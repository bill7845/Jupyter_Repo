{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickles = ['멘탈_관리_심리_탐구?q=g','문화·예술?q=g','건축·설계?q=g'\n",
    "              ,'인문학·철학?q=g','쉽게_읽는_역사?q=g','우리집_반려동물?q=g',\n",
    "                 '멋진_캘리그래피?q=g','사랑·이별?q=g','감성_에세이?q=g']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "멘탈_관리_심리_탐구?q=g\n",
      "문화·예술?q=g\n",
      "건축·설계?q=g\n",
      "인문학·철학?q=g\n",
      "쉽게_읽는_역사?q=g\n",
      "우리집_반려동물?q=g\n",
      "멋진_캘리그래피?q=g\n",
      "사랑·이별?q=g\n",
      "감성_에세이?q=g\n"
     ]
    }
   ],
   "source": [
    "writer_list = []\n",
    "for file in pickles:\n",
    "    print(file)\n",
    "    with open(file[:-4]+\"_userId.txt\",\"rb\") as fr:\n",
    "        writers = pickle.load(fr)\n",
    "    writer_list.append(writers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 게시글 크롤링\n",
    "\n",
    "data = []\n",
    "json_data = {}\n",
    "# res_text = []\n",
    "\n",
    "def def_craw(writer_list,input_catenum):\n",
    "    global json_data\n",
    "    global data\n",
    "    res_text = []\n",
    "    tag_keyword=[]\n",
    "    \n",
    "    tag_title,tag_nickname,tag_publish_date,tag_url,tag_url_plink = None,None,None,None,None\n",
    "    tag_share,tag_like = str,str\n",
    "    for i,url in enumerate(writer_list[input_catenum]):\n",
    "#         print(\"전체%d개의 게시글 중 %d번째 글 진행중\" %(len(writer_list[input_catenum]),i+1))\n",
    "        if res_text == []: # 첫 시작에러 방지\n",
    "            pass\n",
    "        else :\n",
    "            \n",
    "            # to json\n",
    "            json_data['title'] = tag_title  \n",
    "            json_data['nickname'] = tag_nickname\n",
    "            json_data['publish_date'] = tag_publish_date\n",
    "            json_data['keyword'] = tmp_keyword   \n",
    "            json_data['like'] = tag_like # like 없는 경우 ''\n",
    "            json_data['share'] = tag_share # share 없는 경우 None\n",
    "            json_data['comment'] = tag_comment # comment 없는 경우 ''\n",
    "            json_data['url'] = tag_url\n",
    "            json_data['url_plink'] = tag_url_plink \n",
    "            json_data['text'] = res_text\n",
    "\n",
    "        data.append(json_data)\n",
    "#         print(data)\n",
    "        \n",
    "        json_data = {} # 누적방지 초기화\n",
    "        tmp_keyword = [] # 누적방지 초기화\n",
    "        res_text = [] # 누적방지 초기화 \n",
    "        \n",
    "        # beautifulsoup\n",
    "        html = requests.get('https://brunch.co.kr{text_url}'.format(text_url=url))\n",
    "        soup = BeautifulSoup(html.text, 'html.parser')\n",
    "        \n",
    "        if soup.find('title').text == \"brunch\":\n",
    "            pass\n",
    "        else:\n",
    "            tag_title = soup.find('title').text # 해당페이지의 Title\n",
    "            tag_url = soup.find(\"meta\",property='og:url')['content'] # 본주소\n",
    "            tag_nickname = soup.find(\"meta\",{'name':'article:media_name'})['content'] # 글쓴이 닉네임\n",
    "            tag_url_plink = soup.find(\"meta\",property='dg:plink')['content'] # 암호주소? # 모바일?\n",
    "            tag_publish_date = soup.find(\"meta\",property='article:published_time')['content'] # 본주소\n",
    "            tag_keyword = soup.find_all('a',href=re.compile('/keyword')) # 해당 글의 키워드\n",
    "            tag_like = soup.find('span',{'class':'f_l text_like_count text_default text_with_img_ico ico_likeit_like #like'}) #좋아요 수\n",
    "            tag_share = soup.find('span',{'class':'f_l text_share_count text_default text_with_img_ico'}) # 공유 수\n",
    "            tag_comment = soup.find('span',{'class':'f_l text_comment_count text_default text_with_img_ico'}) # 댓글 수\n",
    "            tag_url_meta = soup.find(\"meta\",property='ks:richscrap')['content']\n",
    "        \n",
    "            to_json = json.loads(tag_url_meta)\n",
    "            for i in to_json['cards']:\n",
    "                if i['type'] == 'text':\n",
    "                    res_text.append(i['body']['content'])\n",
    "\n",
    "            if tag_like == None:\n",
    "                tag_like = \"0\"\n",
    "            else:\n",
    "                tag_like = tag_like.text # 좋아요 수\n",
    "\n",
    "            if tag_share == None:\n",
    "                tag_share == \"0\"\n",
    "            else:\n",
    "                tag_share = tag_share.text # 공유 수\n",
    "\n",
    "            if tag_comment == None:\n",
    "                tag_comment ==\"0\"\n",
    "            else:\n",
    "                tag_comment = tag_comment.text\n",
    "\n",
    "            for keyword in tag_keyword:\n",
    "                tmp_keyword.append(keyword.text)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '지구한바퀴_세계여행','그림·웹툰','시사·이슈','IT_트렌드','사진·촬영','',\n",
    "#                  '취향저격_영화_리뷰','오늘은_이런_책','뮤직_인사이드','글쓰기_코치','직장인_현실_조언'\n",
    "#                  ,'스타트업_경험담','육아_이야기','요리·레시피','건강·운동',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "categories = ['멘탈_관리_심리_탐구','문화·예술','건축·설계','인문학·철학','쉽게_읽는_역사',\n",
    "              '우리집_반려동물','멋진_캘리그래피','사랑·이별','감성_에세이']\n",
    "\n",
    "for idx,writer in enumerate(writer_list):\n",
    "    data = def_craw(writer_list,idx)\n",
    "    \n",
    "    del data[0]\n",
    "    del data[0]\n",
    "    to_json = OrderedDict()\n",
    "    full_data = OrderedDict()\n",
    "    \n",
    "    to_json['name'] = categories[idx]\n",
    "    to_json['version'] = \"2020-04-01\"\n",
    "    to_json['data'] = data\n",
    "    \n",
    "    with open(categories[idx]+\".json\",\"w\",encoding='utf-8') as make_file:\n",
    "        json.dump(to_json,make_file,ensure_ascii=False, indent=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
