{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import pickle\n",
    "# import requests\n",
    "# import json\n",
    "# import re\n",
    "\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# ## url pickle load # 건축/설계 제외 우선\n",
    "# pickles = ['지구한바퀴_세계여행?q=g','시사·이슈?q=g','IT_트렌드?q=g',\n",
    "#                  '취향저격_영화_리뷰?q=g','오늘은_이런_책?q=g','뮤직_인사이드?q=g','직장인_현실_조언?q=g'\n",
    "#                  ,'스타트업_경험담?q=g']\n",
    "\n",
    "# writer_list = []\n",
    "# for file in pickles:\n",
    "#     with open(file[:-4]+\"_userId.txt\",\"rb\") as fr:\n",
    "#         writers = pickle.load(fr)\n",
    "#     writer_list.append(writers)  ## [[카테고리1 게시글 url...],[카테고리2 게시글 url], ....[카테고리24 게시글 url]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 12800 중에 0\n",
      "전체 12800 중에 500\n",
      "전체 12800 중에 1000\n",
      "전체 12800 중에 1500\n",
      "전체 12800 중에 2000\n",
      "전체 12800 중에 2500\n",
      "전체 12800 중에 3000\n",
      "전체 12800 중에 3500\n",
      "전체 12800 중에 4000\n",
      "전체 12800 중에 4500\n",
      "전체 12800 중에 5000\n",
      "전체 12800 중에 5500\n",
      "전체 12800 중에 6000\n",
      "전체 12800 중에 6500\n",
      "전체 12800 중에 7000\n",
      "전체 12800 중에 7500\n",
      "전체 12800 중에 8000\n",
      "전체 12800 중에 8500\n",
      "전체 12800 중에 9000\n",
      "전체 12800 중에 9500\n",
      "전체 12800 중에 10000\n",
      "전체 12800 중에 10500\n",
      "전체 12800 중에 11000\n",
      "전체 12800 중에 11500\n",
      "전체 12800 중에 12000\n",
      "전체 12800 중에 12500\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "## url pickle load\n",
    "# pickles = ['IT_트렌드?q=g','취향저격_영화_리뷰?q=g','오늘은_이런_책?q=g','뮤직_인사이드?q=g','직장인_현실_조언?q=g'\n",
    "#                  ,'스타트업_경험담?q=g']\n",
    "pickles = ['스타트업_경험담?q=g']\n",
    "# '지구한바퀴_세계여행?q=g','시사·이슈?q=g','IT_트렌드?q=g','취향저격_영화_리뷰?q=g'\n",
    "\n",
    "\n",
    "writer_list = []\n",
    "for file in pickles:\n",
    "#     print(file)\n",
    "    with open(file[:-4]+\"_userId.txt\",\"rb\") as fr:\n",
    "        writers = pickle.load(fr)\n",
    "    writer_list.append(writers)  ## [[카테고리1 게시글 url...],[카테고리2 게시글 url], ....[카테고리24 게시글 url]]\n",
    "\n",
    "## 게시글 속 정보 수집\n",
    "def def_craw(writer):\n",
    "    \n",
    "    json_data = {}\n",
    "    data = []\n",
    "    res_text = []\n",
    "    tag_keyword=[]\n",
    "    \n",
    "    tag_title,tag_nickname,tag_publish_date,tag_url,tag_url_plink = None,None,None,None,None\n",
    "    tag_share,tag_like = str,str\n",
    "    for idx,url in enumerate(writer):\n",
    "        if idx % 500 == 0 : print(\"전체\",len(writer),\"중에\",idx)\n",
    "        if res_text == []: # 첫 시작에러 방지\n",
    "            pass\n",
    "        else :\n",
    "            # to json\n",
    "            json_data['title'] = tag_title  \n",
    "            json_data['nickname'] = tag_nickname\n",
    "            json_data['publish_date'] = tag_publish_date\n",
    "            json_data['keyword'] = tmp_keyword   \n",
    "            json_data['like'] = tag_like # like 없는 경우 ''\n",
    "            json_data['share'] = tag_share # share 없는 경우 None            \n",
    "            json_data['comment'] = tag_comment # comment 없는 경우 ''\n",
    "            json_data['url'] = tag_url\n",
    "            json_data['url_plink'] = tag_url_plink \n",
    "            json_data['text'] = res_text\n",
    "\n",
    "        data.append(json_data)\n",
    "        \n",
    "        json_data = {} # 누적방지 초기화\n",
    "        tmp_keyword = [] # 누적방지 초기화\n",
    "        res_text = [] # 누적방지 초기화 \n",
    "        \n",
    "        # beautifulsoup\n",
    "        html = requests.get('https://brunch.co.kr{text_url}'.format(text_url=url))\n",
    "        soup = BeautifulSoup(html.text, 'html.parser')\n",
    "        \n",
    "        if soup.find('title').text == \"brunch\":\n",
    "            pass\n",
    "        else:\n",
    "            tag_title = soup.find('title').text # 게시글 title\n",
    "            tag_url = soup.find(\"meta\",property='og:url')['content'] # 게시글 본주소\n",
    "            tag_nickname = soup.find(\"meta\",{'name':'article:media_name'})['content'] # 작가 nickname\n",
    "            tag_url_plink = soup.find(\"meta\",property='dg:plink')['content'] # 암호주소? # 모바일?\n",
    "            tag_publish_date = soup.find(\"meta\",property='article:published_time')['content'] # 발행일\n",
    "            tag_keyword = soup.find_all('a',href=re.compile('/keyword')) # 게시글 키워드\n",
    "            tag_like = soup.find('span',{'class':'f_l text_like_count text_default text_with_img_ico ico_likeit_like #like'}) #좋아요 수\n",
    "            tag_share = soup.find('span',{'class':'f_l text_share_count text_default text_with_img_ico'}) # 공유 수\n",
    "            tag_comment = soup.find('span',{'class':'f_l text_comment_count text_default text_with_img_ico'}) # 댓글 수\n",
    "            text_h4 = soup.find_all(class_='wrap_item item_type_text')\n",
    "            \n",
    "            for text in text_h4:\n",
    "                res_text.append(text.text)\n",
    "    \n",
    "            if tag_like == None:\n",
    "                tag_like = \"0\"\n",
    "            else:\n",
    "                tag_like = tag_like.text # 좋아요 수\n",
    "\n",
    "            if tag_share == None:\n",
    "                tag_share == \"0\"\n",
    "            else:\n",
    "                tag_share = tag_share.text # 공유 수\n",
    "\n",
    "            if tag_comment == None:\n",
    "                tag_comment ==\"0\"\n",
    "            else:\n",
    "                tag_comment = tag_comment.text\n",
    "\n",
    "            for keyword in tag_keyword:\n",
    "                tmp_keyword.append(keyword.text)\n",
    "                \n",
    "    return data ## 수집한 정보를 담은 dictionary로 반환\n",
    "\n",
    "\n",
    "categories = ['스타트업_경험담']\n",
    "# categories = ['지구한바퀴_세계여행,','시사_이슈','IT_트렌드','취향저격_영화리뷰','오늘은_이런책',\n",
    "#              '뮤직_인사이드','직장인_현실조언','스타트업_경험담']\n",
    "\n",
    "\n",
    "## 카테고리 -> 게시글의 순서로 2차 크롤링 진행\n",
    "## 카테고리별로 정보를 담은 json 형식으로 저장(총 24개 json file)\n",
    "from collections import OrderedDict\n",
    "for idx,writer in enumerate(writer_list):\n",
    "    to_json = None\n",
    "    data = def_craw(writer) # 2단계 크롤링 실행\n",
    "    \n",
    "    del data[0]\n",
    "    del data[0]\n",
    "    \n",
    "    to_json = OrderedDict()\n",
    "    to_json['name'] = categories[idx] # category name\n",
    "    to_json['version'] = \"2020-05-19\"\n",
    "    to_json['data'] = data\n",
    "    \n",
    "    with open(categories[idx]+\".json\",\"w\") as make_file:\n",
    "        json.dump(to_json,make_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
