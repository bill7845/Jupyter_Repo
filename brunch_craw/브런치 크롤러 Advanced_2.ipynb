{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "## url pickle load\n",
    "pickles = ['지구한바퀴_세계여행?q=g','그림·웹툰?q=g','시사·이슈?q=g','IT_트렌드?q=g','사진·촬영?q=g',\n",
    "                 '취향저격_영화_리뷰?q=g','오늘은_이런_책?q=g','뮤직_인사이드?q=g','글쓰기_코치?q=g','직장인_현실_조언?q=g'\n",
    "                 ,'스타트업_경험담?q=g','육아_이야기?q=g','요리·레시피?q=g','건강·운동?q=g','멘탈_관리_심리_탐구?q=g',\n",
    "                 '디자인_스토리?q=g','문화·예술?q=g','건축·설계?q=g','인문학·철학?q=g','쉽게_읽는_역사?q=g','우리집_반려동물?q=g',\n",
    "                 '멋진_캘리그래피?q=g','사랑·이별?q=g','감성_에세이?q=g']\n",
    "\n",
    "writer_list = []\n",
    "for file in pickles:\n",
    "#     print(file)\n",
    "    with open('C:/Users/KIHyuk/Desktop/brunch_data/user_id/'+file[:-4]+\"_userId.txt\",\"rb\") as fr:\n",
    "        writers = pickle.load(fr)\n",
    "    writer_list.append(writers)  ## [[카테고리1 게시글 url...],[카테고리2 게시글 url], ....[카테고리24 게시글 url]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "## url pickle load\n",
    "pickles = ['지구한바퀴_세계여행?q=g','그림·웹툰?q=g','시사·이슈?q=g','IT_트렌드?q=g','사진·촬영?q=g',\n",
    "                 '취향저격_영화_리뷰?q=g','오늘은_이런_책?q=g','뮤직_인사이드?q=g','글쓰기_코치?q=g','직장인_현실_조언?q=g'\n",
    "                 ,'스타트업_경험담?q=g','육아_이야기?q=g','요리·레시피?q=g','건강·운동?q=g','멘탈_관리_심리_탐구?q=g',\n",
    "                 '디자인_스토리?q=g','문화·예술?q=g','건축·설계?q=g','인문학·철학?q=g','쉽게_읽는_역사?q=g','우리집_반려동물?q=g',\n",
    "                 '멋진_캘리그래피?q=g','사랑·이별?q=g','감성_에세이?q=g']\n",
    "\n",
    "writer_list = []\n",
    "for file in pickles:\n",
    "#     print(file)\n",
    "    with open('C:/Users/KIHyuk/Desktop/brunch_data/user_id/'+file[:-4]+\"_userId.txt\",\"rb\") as fr:\n",
    "        writers = pickle.load(fr)\n",
    "    writer_list.append(writers)  ## [[카테고리1 게시글 url...],[카테고리2 게시글 url], ....[카테고리24 게시글 url]]\n",
    "\n",
    "## 게시글 속 정보 수집\n",
    "def def_craw(writer):\n",
    "    json_data = {}\n",
    "    data = []\n",
    "    res_text = []\n",
    "    tag_keyword=[]\n",
    "    \n",
    "    tag_title,tag_nickname,tag_publish_date,tag_url,tag_url_plink = None,None,None,None,None\n",
    "    tag_share,tag_like = str,str\n",
    "    for url in writer:\n",
    "        if res_text == []: # 첫 시작에러 방지\n",
    "            pass\n",
    "        else :\n",
    "            # to json\n",
    "            json_data['title'] = tag_title  \n",
    "            json_data['nickname'] = tag_nickname\n",
    "            json_data['publish_date'] = tag_publish_date\n",
    "            json_data['keyword'] = tmp_keyword   \n",
    "            json_data['like'] = tag_like # like 없는 경우 ''\n",
    "            json_data['share'] = tag_share # share 없는 경우 None\n",
    "            json_data['comment'] = tag_comment # comment 없는 경우 ''\n",
    "            json_data['url'] = tag_url\n",
    "            json_data['url_plink'] = tag_url_plink \n",
    "            json_data['text'] = res_text\n",
    "\n",
    "        data.append(json_data)\n",
    "        \n",
    "        json_data = {} # 누적방지 초기화\n",
    "        tmp_keyword = [] # 누적방지 초기화\n",
    "        res_text = [] # 누적방지 초기화 \n",
    "        \n",
    "        # beautifulsoup\n",
    "        html = requests.get('https://brunch.co.kr{text_url}'.format(text_url=url))\n",
    "        soup = BeautifulSoup(html.text, 'html.parser')\n",
    "        \n",
    "        if soup.find('title').text == \"brunch\":\n",
    "            pass\n",
    "        else:\n",
    "            tag_title = soup.find('title').text # 게시글 title\n",
    "            tag_url = soup.find(\"meta\",property='og:url')['content'] # 게시글 본주소\n",
    "            tag_nickname = soup.find(\"meta\",{'name':'article:media_name'})['content'] # 작가 nickname\n",
    "            tag_url_plink = soup.find(\"meta\",property='dg:plink')['content'] # 암호주소? # 모바일?\n",
    "            tag_publish_date = soup.find(\"meta\",property='article:published_time')['content'] # 발행일\n",
    "            tag_keyword = soup.find_all('a',href=re.compile('/keyword')) # 게시글 키워드\n",
    "            tag_like = soup.find('span',{'class':'f_l text_like_count text_default text_with_img_ico ico_likeit_like #like'}) #좋아요 수\n",
    "            tag_share = soup.find('span',{'class':'f_l text_share_count text_default text_with_img_ico'}) # 공유 수\n",
    "            tag_comment = soup.find('span',{'class':'f_l text_comment_count text_default text_with_img_ico'}) # 댓글 수\n",
    "            text_h4 = soup.find_all(class_='wrap_item item_type_text')\n",
    "            \n",
    "            for text in text_h4:\n",
    "                res_text.append(text.text)\n",
    "    \n",
    "            if tag_like == None:\n",
    "                tag_like = \"0\"\n",
    "            else:\n",
    "                tag_like = tag_like.text # 좋아요 수\n",
    "\n",
    "            if tag_share == None:\n",
    "                tag_share == \"0\"\n",
    "            else:\n",
    "                tag_share = tag_share.text # 공유 수\n",
    "\n",
    "            if tag_comment == None:\n",
    "                tag_comment ==\"0\"\n",
    "            else:\n",
    "                tag_comment = tag_comment.text\n",
    "\n",
    "            for keyword in tag_keyword:\n",
    "                tmp_keyword.append(keyword.text)\n",
    "                \n",
    "    return data ## 수집한 정보를 담은 dictionary로 반환\n",
    "\n",
    "# categories = ['지구한바퀴_세계여행','그림·웹툰','시사·이슈','IT_트렌드','사진·촬영',\n",
    "#                  '취향저격_영화_리뷰','오늘은_이런_책','뮤직_인사이드','글쓰기_코치','직장인_현실_조언'\n",
    "#                  ,'스타트업_경험담','육아_이야기','요리·레시피','건강·운동','멘탈_관리_심리_탐구',\n",
    "#                  '디자인_스토리','문화·예술','건축·설계','인문학·철학','쉽게_읽는_역사','우리집_반려동물',\n",
    "#                  '멋진_캘리그래피','사랑·이별','감성_에세이']\n",
    "\n",
    "\n",
    "## 카테고리 -> 게시글의 순서로 2차 크롤링 진행\n",
    "## 카테고리별로 정보를 담은 json 형식으로 저장(총 24개 json file)\n",
    "for idx,writer in enumerate(writer_list):\n",
    "    to_json = None\n",
    "    data = def_craw(writer) # 2단계 크롤링 실행\n",
    "    \n",
    "    del data[0]\n",
    "    del data[0]\n",
    "    \n",
    "    to_json = OrderedDict()\n",
    "    to_json['name'] = categories[idx] # category name\n",
    "    to_json['version'] = \"2020-04-21\"\n",
    "    to_json['data'] = data\n",
    "    \n",
    "    with open(categories[idx]+\".json\",\"w\") as make_file:\n",
    "        json.dump(to_json,make_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
