{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
      "Requirement already satisfied: urllib3 in c:\\anaconda3\\lib\\site-packages (from selenium) (1.24.2)\n",
      "Installing collected packages: selenium\n",
      "Successfully installed selenium-3.141.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.3; however, version 19.3.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import NoAlertPresentException\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import time\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import defaultdict\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brunch data crawling by 셀레니엄\n",
    "# source reference : http://stackoverflow.com/questions/12519074/scrape-websites-with-infinite-scrolling\n",
    "\n",
    "# category_list = ['지구한바퀴_세계여행?q=g','그림·웹툰?q=g','시사·이슈?q=g','IT_트렌드?q=g','사진·촬영?q=g','',\n",
    "#                  '취향저격_영화_리뷰?q=g','오늘은_이런_책?q=g','뮤직_인사이드?q=g','글쓰기_코치?q=g','직장인_현실_조언?q=g'\n",
    "#                  ,'스타트업_경험담?q=g','육아_이야기?q=g','요리·레시피?q=g','건강·운동?q=g','멘탈_관리_심리_탐구?q=g',\n",
    "#                  '디자인_스토리?q=g','문화·예술?q=g','건축·설계?q=g','인문학·철학?q=g','쉽게_읽는_역사?q=g','우리집_반려동물?q=g',\n",
    "#                  '멋진_캘리그래피?q=g','사랑·이별?q=g','감성_에세이?q=g']\n",
    "\n",
    "category_list = ['지구한바퀴_세계여행?q=g','그림·웹툰?q=g']\n",
    "idlist = []\n",
    "\n",
    "def crawlBrunchLink(dir):\n",
    "    ## html crawling\n",
    "    chromedriver = 'C:/selenium/chromedriver.exe'\n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "    #driver.implicitly_wait(30)\n",
    "    base_url = \"https://brunch.co.kr/keyword/{dir}\".format(dir=dir)\n",
    "    verificationErrors = []\n",
    "    accept_next_alert = True\n",
    "    delay = 0.5\n",
    "    driver.get(base_url)\n",
    "    #driver.find_element_by_link_text(\"All\").click()\n",
    "    htmlsize = 0\n",
    "    keep_cnt = 0\n",
    "    for i in range(1,2): # 스크롤 횟수\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\") # 페이지 무한 스크롤\n",
    "        time.sleep(1)\n",
    "        \n",
    "        if htmlsize == len(driver.page_source):\n",
    "            keep_cnt += 1\n",
    "        else :\n",
    "            keep_cnt = 0\n",
    "            htmlsize = len(driver.page_source)\n",
    "        if keep_cnt > 5 :\n",
    "            break\n",
    "            \n",
    "    html_source = driver.page_source\n",
    "    ## extract follower, following data\n",
    "    data = html_source.encode('utf-8')\n",
    "\n",
    "    soup = BeautifulSoup(data, 'html.parser') \n",
    "    classes = soup.find_all('a', href=re.compile('/@@')) # /@@ 로 시작하는 href 태그 찾아오기\n",
    "#     idlist = []\n",
    "    for c in classes:\n",
    "        follwing = c.get('href')\n",
    "        if follwing is None or len(follwing)==0:\n",
    "            continue\n",
    "        idlist.append(follwing[2:])\n",
    "\n",
    "    driver.close()\n",
    "    return idlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "writer_list = [] # 크롤링 해온 해당글 url 리스트\n",
    "\n",
    "for dir in category_list:\n",
    "    idlist =[] # idlist 누적방지\n",
    "    crawlBrunchLink(dir)\n",
    "#     print(idlist)\n",
    "    writer_list.append(idlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "data = []\n",
    "json_data = {}\n",
    "\n",
    "def def_craw(writer_list,input_catenum):\n",
    "    global json_data\n",
    "    global data\n",
    "    tmp_text = []\n",
    "    tag_keyword=[]\n",
    "    \n",
    "    tag_title,tag_nickname,tag_publish_date,tag_like,tag_share,tag_url,tag_url_plink = None,None,None,None,None,None,None\n",
    "    for url in (writer_list[input_catenum]):\n",
    "        \n",
    "        if tmp_text == []:\n",
    "            pass\n",
    "        else :\n",
    "            json_data['title'] = tag_title.text  \n",
    "#             json_data['nickname'] = tag_nickname\n",
    "            json_data['publish_date'] = tag_publish_date\n",
    "            json_data['keyword'] = tmp_keyword   \n",
    "            json_data['like'] = tag_like\n",
    "#             json_data['share'] = tag_share\n",
    "            json_data['url'] = tag_url\n",
    "            json_data['url_plink'] = tag_url_plink \n",
    "            json_data['text'] = tmp_text\n",
    "\n",
    "\n",
    "        data.append(json_data)\n",
    "        \n",
    "        json_data = {} # 초기화\n",
    "        tmp_text = [] # 초기화\n",
    "        tmp_keyword = [] # 초기화\n",
    "        \n",
    "        \n",
    "        # beautifulsoup \n",
    "        html = requests.get('https://brunch.co.kr/@{text_url}'.format(text_url=url))\n",
    "        soup = BeautifulSoup(html.text, 'html.parser')\n",
    "        tag_text = soup.find_all('p') # 해당페이지의 본문\n",
    "        tag_title = soup.find('title') # 해당페이지의 Title\n",
    "#         tag_nickname = soup.find(\"meta\",{'name':'article:media_name'})['content'] # 글쓴이 닉네임\n",
    "        tag_url = soup.find(\"meta\",property='og:url')['content'] # 본주소\n",
    "        tag_url_plink = soup.find(\"meta\",property='dg:plink')['content'] # 암호주소? # 모바일?\n",
    "        tag_publish_date = soup.find(\"meta\",property='article:published_time')['content'] # 본주소\n",
    "        tag_keyword = soup.find_all('a',href=re.compile('/keyword')) # 해당 글의 키워드\n",
    "        tag_like = soup.find('span',{'class':'f_l text_like_count text_default text_with_img_ico ico_likeit_like #like'}).text # 좋아요 수\n",
    "#         tag_share = soup.find('span',{'class':'f_l text_share_count text_default text_with_img_ico'}).text # 공유 수\n",
    "        \n",
    "        for text in tag_text:\n",
    "            tmp_text.append(text.text) # p태그 text\n",
    "        for keyword in tag_keyword:\n",
    "            tmp_keyword.append(keyword.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-330-6b48dfb72543>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdef_craw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwriter_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-329-59570db6c051>\u001b[0m in \u001b[0;36mdef_craw\u001b[1;34m(writer_list, input_catenum)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;31m#         tag_nickname = soup.find(\"meta\",{'name':'article:media_name'})['content'] # 글쓴이 닉네임\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mtag_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"meta\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'og:url'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'content'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# 본주소\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mtag_url_plink\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"meta\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'dg:plink'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'content'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# 암호주소? # 모바일?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0mtag_publish_date\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"meta\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'article:published_time'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'content'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# 본주소\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mtag_keyword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/keyword'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 해당 글의 키워드\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "def_craw(writer_list,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get('https://brunch.co.kr/@{text_url}'.format(text_url='@7LuB/12'))\n",
    "soup = BeautifulSoup(html.text, 'html.parser')\n",
    "tag_title = soup.find_all('p') # 해당페이지의 Title \n",
    "tag_titlse = soup.find('title') # 해당페이지의 Title\n",
    "tag_nickname = soup.find(\"meta\",{'name':'article:media_name'})['content'] # 글쓴이 닉네임\n",
    "tag_url = soup.find(\"meta\",property='og:url')['content'] # 본주소\n",
    "tag_url_plink = soup.find(\"meta\",property='dg:plink')['content'] # 암호주소? # 모바일?\n",
    "tag_publish_date = soup.find(\"meta\",property='article:published_time')['content'] # 본주소\n",
    "tag_keyword = soup.find_all('a',href=re.compile('/keyword')) # 해당 글의 키워드\n",
    "tag_like = soup.find('span',{'class':'f_l text_like_count text_default text_with_img_ico ico_likeit_like #like'}).text # 좋아요 수\n",
    "tag_share = soup.find('span',{'class':'f_l text_share_count text_default text_with_img_ico'}).text # 공유 수\n",
    "# tag_comment = soup.find('span',{'class':'f_l text_comment_count text_default text_with_img_ico'}) # 댓글 수는 이미지임\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11'"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_share"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
