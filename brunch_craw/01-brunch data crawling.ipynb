{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### brunch data crawling\n",
    "* 작가들의 Followers 테이블을 만들다 \n",
    "* Followers에서 following을 크롤링할 대상을 SQL 쿼리로 산출한다. \n",
    "* folling 테이블에 추가한다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 브런치 Follower(구독자), Following(관심작가) 정보 수집 \n",
    "```html\n",
    "<ul class=\"list_follow\">객체에서\n",
    "a tag href=\"/@\"으로 시작하는 아이디 가져오기\n",
    "```\n",
    "<img src='https://t1.daumcdn.net/thumb/R1280x0/?fname=http://t1.daumcdn.net/brunch/service/user/Kvs/image/vS4eDK8M1R8ytc0HUJTlG8Vk-jg.png'>\n",
    "\n",
    "* crawling 절차\n",
    " * https://brunch.co.kr/@{user-id}/{following} URL로 HTML을 크롤링한다. \n",
    "    : brunch 관심작가, 구독자가 공개는 되어 있지만, 무한 스크롤 방식의 UX를 사용하기 때문에 셀리늄2.0(webdriver)를 통해 html을 크롤링할수 있다. \n",
    " * 크롤링된 HTML을 BS4로 class = link_follow 이고 herf = '@****' 로 시작되는 아이디만 추출\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "WebDriverException",
     "evalue": "Message: 'geckodriver' executable needs to be in PATH. \n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\common\\service.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     75\u001b[0m                                             \u001b[0mstderr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m                                             stdin=PIPE)\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[0;32m    774\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 775\u001b[1;33m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[0;32m    776\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1177\u001b[0m                                          \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcwd\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m                                          startupinfo)\n\u001b[0m\u001b[0;32m   1179\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] 지정된 파일을 찾을 수 없습니다",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-832729d1764b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# brunch data crawling by 셀레니엄\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# source reference : http://stackoverflow.com/questions/12519074/scrape-websites-with-infinite-scrolling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mcrawlBrunchLink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'follower'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFirefox\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;31m## html crawling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://brunch.co.kr/@{uid}/{dir}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\firefox\\webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, firefox_profile, firefox_binary, timeout, capabilities, proxy, executable_path, options, service_log_path, firefox_options, service_args, desired_capabilities, log_path, keep_alive)\u001b[0m\n\u001b[0;32m    162\u001b[0m                 \u001b[0mservice_args\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mservice_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m                 log_path=service_log_path)\n\u001b[1;32m--> 164\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mservice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m             \u001b[0mcapabilities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_capabilities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\common\\service.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     81\u001b[0m                 raise WebDriverException(\n\u001b[0;32m     82\u001b[0m                     \"'%s' executable needs to be in PATH. %s\" % (\n\u001b[1;32m---> 83\u001b[1;33m                         os.path.basename(self.path), self.start_error_message)\n\u001b[0m\u001b[0;32m     84\u001b[0m                 )\n\u001b[0;32m     85\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEACCES\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mWebDriverException\u001b[0m: Message: 'geckodriver' executable needs to be in PATH. \n"
     ]
    }
   ],
   "source": [
    "# brunch data crawling by 셀레니엄\n",
    "# source reference : http://stackoverflow.com/questions/12519074/scrape-websites-with-infinite-scrolling\n",
    "def crawlBrunchLink(uid, dir='follower', driver=webdriver.Firefox()):\n",
    "    ## html crawling\n",
    "    url = \"https://brunch.co.kr/@{uid}/{dir}\".format(uid=uid, dir=dir)\n",
    "    driver.get(url)\n",
    "\n",
    "    htmlsize = 0\n",
    "    keep_cnt = 0\n",
    "    for i in range(1,200):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(0.003) \n",
    "        if htmlsize == len(driver.page_source):\n",
    "            keep_cnt += 1\n",
    "        else :\n",
    "            keep_cnt = 0\n",
    "            htmlsize = len(driver.page_source)\n",
    "        if keep_cnt > 5 :\n",
    "            break\n",
    "            \n",
    "    html_source = driver.page_source\n",
    "    ## extract follower, following data\n",
    "    data = html_source.encode('utf-8')\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "    classes = soup.find_all(\"a\", class_=\"link_follow\")\n",
    "    idlist = []\n",
    "    for c in classes:\n",
    "        follwing = c.get('href')\n",
    "        if follwing is None or len(follwing)==0:\n",
    "            continue\n",
    "        idlist.append(follwing[2:])\n",
    "\n",
    "    #driver.close()\n",
    "    return idlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 관심 작가 정보 크롤링 하기 \n",
    "## extract Brunch Writer Info : uid, name, text-count, megazine-count, follower-count, following-count:\n",
    "def extractWriterInfo(uid):\n",
    "    try:\n",
    "        response = requests.get(\"http://brunch.co.kr/@{uid}\".format(uid=uid) )\n",
    "    except Exception:\n",
    "        []\n",
    "        \n",
    "    data = response.content.decode('utf-8')\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "    \n",
    "    ## name \n",
    "    names = soup.find_all(\"strong\", class_=\"profileUserName\")\n",
    "    name = uid if len(names)<1 else names[0].getText()\n",
    "    \n",
    "    ## profile description\n",
    "    desc = soup.find_all(\"pre\", class_=\"profileUserDesc\")\n",
    "    desc = \"{} 브런치입니다.\".format(uid) if len(desc)<1 else desc[0].getText()\n",
    "    \n",
    "    ## thumbnail image link\n",
    "    imgsrc = soup.find_all(\"input\", class_=\"profileUserImageUrl\")\n",
    "    imgsrc = \"no-img\" if len(imgsrc)<1 else imgsrc[0].get('value')\n",
    "    \n",
    "    classes = soup.find_all(\"span\", class_=\"num_count\")\n",
    "    reserved = [uid, name, desc, imgsrc]\n",
    "    for c in classes:\n",
    "        reserved.append(int(c.getText().replace(\",\",\"\")))\n",
    "    \n",
    "    if len(reserved) < 8:\n",
    "        for n in range(0,8-(len(reserved))):\n",
    "            reserved.append(0)\n",
    "    return reserved[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터의 수집 방법\n",
    "* goodvc78의 follower부터 시작하여 재귀적으로 데이터 수집 \n",
    "* goodvc78의 follower수집(1) --> follower들의 following 작가 수집(2) --> following 작가들의 follower수집(1) --> (2)\n",
    "\n",
    "                         \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertData(tbl_name, columns, rows):\n",
    "    conn = sqlite3.connect('/Users/goodvc/Documents/data/sqllite/brunch_db.db')\n",
    "    col_str = \", \".join(columns)\n",
    "    val_str = \", \".join(['?' for n in columns])\n",
    "\n",
    "    sql = \"insert into {tbl} ({cols}) values ({vals}) \".format(tbl=tbl_name, cols=col_str, vals=val_str)\n",
    "    try:\n",
    "        ret = conn.executemany(sql, rows)\n",
    "    except Exception:\n",
    "        conn.rollback()\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "### sample code \n",
    "#now = (int(time.time()))\n",
    "#rows = [['goodvc78', 'test1', now ],['goodvc78', 'test2', now ]]    \n",
    "#insertData( 'follower_tbl', ['writerid', 'userid', 'tm'], rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertFollowings(base, id_list):\n",
    "    now = (int(time.time()))\n",
    "    rows = [[base, uid, now] for uid in id_list]\n",
    "    insertData( 'following_tbl', ['userid', 'writerid', 'tm'], rows)\n",
    "    \n",
    "def insertFollowers(base, id_list):\n",
    "    now = (int(time.time()))\n",
    "    rows = [[base, uid, now] for uid in id_list]\n",
    "    insertData( 'follower_tbl', ['writerid','userid',  'tm'], rows)\n",
    "\n",
    "def insertWriterInfo(writer_info_list):\n",
    "    now = (int(time.time()))\n",
    "    rows = []\n",
    "    for info in writer_info_list:\n",
    "        info.append(now)\n",
    "        rows.append(info)\n",
    "    colnames = ['writerid', 'name', 'profile', 'imgsrc', 'documents', 'megazines', 'followers', 'followings', 'tm']\n",
    "    insertData( 'writer_info_tbl', colnames, rows)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### goodvc78's follower list 수집 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. goodvc78's follower crawling \n",
    "base = 'goodvc78'\n",
    "driver = webdriver.Firefox()\n",
    "base_follower = crawlBrunchLink(base, dir='follower', driver=driver)\n",
    "print (\"내가 좋아하는 작가의 followers = %d\" %  len(base_follower) )\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. goodvc78 follower list insert \n",
    "insertFollowers(base, base_follower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unreadUserid(limit=100):\n",
    "    conn = sqlite3.connect('/Users/goodvc/Documents/data/sqllite/brunch_db.db')\n",
    "    sql = \"\"\" \n",
    "    select userid from follower_tbl \n",
    "    where userid not in ( select userid from following_tbl) limit {0};\"\"\".format(limit)\n",
    "    \n",
    "    ds = pd.read_sql(sql, conn)\n",
    "    conn.close()\n",
    "    return ds.userid.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unreadWriterid(limit=10):\n",
    "    conn = sqlite3.connect('/Users/goodvc/Documents/data/sqllite/brunch_db.db')\n",
    "    sql = \"\"\" \n",
    "    select writerid,count(1) cnt from following_tbl\n",
    "    where writerid not in ( select writerid from follower_tbl) and writerid !='brunch'\n",
    "    group by writerid \n",
    "    having cnt > 20 \n",
    "    limit {0};\"\"\".format(limit)\n",
    "    \n",
    "    ds = pd.read_sql(sql, conn)\n",
    "    conn.close()\n",
    "    return ds.writerid.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unreadWriterInfoid(limit=100):\n",
    "    conn = sqlite3.connect('/Users/goodvc/Documents/data/sqllite/brunch_db.db')\n",
    "    sql = \"\"\" \n",
    "    select writerid, count(1) cnt from following_tbl\n",
    "    where writerid not in ( select writerid from writer_info_tbl) and writerid !='brunch'\n",
    "    group by writerid \n",
    "    having cnt > 1 \n",
    "    limit {0};\"\"\".format(limit)\n",
    "    \n",
    "    ds = pd.read_sql(sql, conn)\n",
    "    conn.close()\n",
    "    return ds.writerid.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawlFollowing(limit=100):\n",
    "    driver = webdriver.Firefox()\n",
    "    users = unreadUserid(limit)\n",
    "    print (\"\\ncrawling users \", len(users))\n",
    "    for uid in users :\n",
    "        following = crawlBrunchLink(uid, dir='following', driver=driver)\n",
    "        insertFollowings(uid, following)\n",
    "        print('.',end=\"\")\n",
    "    driver.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawlFollower(limit=10):\n",
    "    driver = webdriver.Firefox()\n",
    "    writers = unreadWriterid(limit)\n",
    "    print (\"\\ncrawling writers \", len(writers))\n",
    "    for writerid in writers :\n",
    "        follower = crawlBrunchLink(writerid, dir='follower', driver=driver)\n",
    "        insertFollowers(writerid, follower)\n",
    "        print('.',end=\"\")\n",
    "    driver.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawlWriterInfo(limit=100):\n",
    "    writers = unreadWriterInfoid(limit)\n",
    "    print (\"\\ncrawling writer info \", len(writers))\n",
    "    infos = []\n",
    "    for writerid in writers :\n",
    "        info = extractWriterInfo(writerid)\n",
    "        if len(info)!=8:\n",
    "            print(\"skipped:{id} {val}\".format(id=writerid, val=info))\n",
    "            continue\n",
    "        infos.append(info)\n",
    "        print('.',end=\"\")\n",
    "    insertWriterInfo(infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## following list crawling\n",
    "for n in range(1,10):\n",
    "    crawlFollowing(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawlFollower(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## following list crawling\n",
    "for n in range(1,100):\n",
    "    crawlFollowing(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## writer info crawling\n",
    "for n in range(1,2):\n",
    "    crawlWriterInfo(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## following list crawling\n",
    "for n in range(1,100):\n",
    "    crawlFollowing(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    crawlFollowing(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
