{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import NoAlertPresentException\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import time\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import defaultdict\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brunch data crawling by 셀레니엄\n",
    "# source reference : http://stackoverflow.com/questions/12519074/scrape-websites-with-infinite-scrolling\n",
    "\n",
    "# category_list = ['지구한바퀴_세계여행?q=g','그림·웹툰?q=g','시사·이슈?q=g','IT_트렌드?q=g','사진·촬영?q=g','',\n",
    "#                  '취향저격_영화_리뷰?q=g','오늘은_이런_책?q=g','뮤직_인사이드?q=g','글쓰기_코치?q=g','직장인_현실_조언?q=g'\n",
    "#                  ,'스타트업_경험담?q=g','육아_이야기?q=g','요리·레시피?q=g','건강·운동?q=g','멘탈_관리_심리_탐구?q=g',\n",
    "#                  '디자인_스토리?q=g','문화·예술?q=g','건축·설계?q=g','인문학·철학?q=g','쉽게_읽는_역사?q=g','우리집_반려동물?q=g',\n",
    "#                  '멋진_캘리그래피?q=g','사랑·이별?q=g','감성_에세이?q=g']\n",
    "\n",
    "category_list = ['지구한바퀴_세계여행?q=g','그림·웹툰?q=g']\n",
    "idlist = []\n",
    "\n",
    "def crawlBrunchLink(dir):\n",
    "    ## html crawling\n",
    "    chromedriver = 'C:/selenium/chromedriver.exe'\n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "    #driver.implicitly_wait(30)\n",
    "    base_url = \"https://brunch.co.kr/keyword/{dir}\".format(dir=dir)\n",
    "    verificationErrors = []\n",
    "    accept_next_alert = True\n",
    "    delay = 0.5\n",
    "    driver.get(base_url)\n",
    "    #driver.find_element_by_link_text(\"All\").click()\n",
    "    htmlsize = 0\n",
    "    keep_cnt = 0\n",
    "    for i in range(1,2):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(1)\n",
    "        \n",
    "        if htmlsize == len(driver.page_source):\n",
    "            keep_cnt += 1\n",
    "        else :\n",
    "            keep_cnt = 0\n",
    "            htmlsize = len(driver.page_source)\n",
    "        if keep_cnt > 5 :\n",
    "            break\n",
    "            \n",
    "    html_source = driver.page_source\n",
    "    ## extract follower, following data\n",
    "    data = html_source.encode('utf-8')\n",
    "\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "    classes = soup.find_all('a', href=re.compile('/@@'))\n",
    "#     idlist = []\n",
    "    for c in classes:\n",
    "        follwing = c.get('href')\n",
    "        if follwing is None or len(follwing)==0:\n",
    "            continue\n",
    "        idlist.append(follwing[2:])\n",
    "\n",
    "    driver.close()\n",
    "    return idlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_list = []\n",
    "\n",
    "for dir in category_list:\n",
    "    crawlBrunchLink(dir)\n",
    "    writer_list.append(idlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from multiprocessing import Pool, Manager\n",
    "# pool = Pool(processes=4)\n",
    "\n",
    "\n",
    "# # manager = Manager()\n",
    "# # res_dict = manager.dict()\n",
    "\n",
    "# def pool_def(category_list):\n",
    "#     res_dict = {}\n",
    "#     tmp_set=[]\n",
    "#     for i,name in enumerate(category_list):\n",
    "#         print(\"%d\"%i)\n",
    "#         res_dict[name] = tmp\n",
    "#         for text_url in writer_list[i]:\n",
    "#             tmp.append(tmp_set)\n",
    "#             tmp_set = []\n",
    "#             html = requests.get('https://brunch.co.kr/@{text_url}'.format(text_url=text_url))\n",
    "#             soup = BeautifulSoup(html.text, 'html.parser')\n",
    "#             ss = soup.find_all('p')\n",
    "#             tmp_set = []\n",
    "#             for j in ss:\n",
    "#                 tmp_set.append(j.text)\n",
    "#     return res_dict\n",
    "    \n",
    "# pool.map(pool_def,category_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<=' not supported between instances of 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-5e30880c190e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mnum_processors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocesses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_processors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool_def\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mcategory_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwriter_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         '''\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36m_map_async\u001b[1;34m(self, func, iterable, mapper, chunksize, callback, error_callback)\u001b[0m\n\u001b[0;32m    392\u001b[0m         \u001b[0mtask_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tasks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         result = MapResult(self._cache, chunksize, len(iterable), callback,\n\u001b[1;32m--> 394\u001b[1;33m                            error_callback=error_callback)\n\u001b[0m\u001b[0;32m    395\u001b[0m         self._taskqueue.put(\n\u001b[0;32m    396\u001b[0m             (\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, cache, chunksize, length, callback, error_callback)\u001b[0m\n\u001b[0;32m    680\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_chunksize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 682\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    683\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_number_left\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    684\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '<=' not supported between instances of 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool, Manager\n",
    "import pool_def\n",
    "\n",
    "if __name__ ==  '__main__': \n",
    "    num_processors = 4\n",
    "    p=Pool(processes = num_processors)\n",
    "    p.map(pool_def.pool_def ,category_list, writer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You can make anythingby writing',\n",
       " '- C.S.Lewis -',\n",
       " '사파, 힐링여행, 숙소추천\\xa0',\n",
       " '베트남 여행이 뜨면서 하노이 근교 작은 마을들이 눈길을 끌고 있는데요,',\n",
       " '이중 SNS에서 가장 핫한 마을은\\xa0자연 그대로의 모습을 간직한 사파!',\n",
       " '두 달 전 예약은 필수라는\\xa0에포 팜스 하우스 - 사파 리트리트를 소개하겠습니다\\xa0:)',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '에코 팜스 하우스 – 사파 리트리트',\n",
       " '',\n",
       " '가장 인기 있는 객실은 탁 트인 전망을 고스란히 즐길 수 있는 ‘방갈로’입니다. 인스타그램에서 보신 모든 인생샷이 탄생한 장소예요. 친환경 하우스지만 커피포트, 냉난방 시설, 금고, 멀티탭, wi-fi 등 현대 시설도 잘 준비되어 있어요. 모기장과 우산, 옷걸이는 기본! 함정은 냉장고가 없어요. \\xa0간식 챙겨 오셔도 소용없습니다.',\n",
       " '',\n",
       " '',\n",
       " '기본 세면도구와 수건, 슬리퍼, 헤어드라이어 등이 완벽하게 갖춰져 있어요. 샴푸와 바디워시는 향이 너무 좋아서 챙겨 오고 싶을 정도예요. 머릿결이 건조하신 분들은 린스나 컨디셔너만 챙겨주세요. 다만 모든 방갈로가 욕조를 갖추고 있지는 않아요. 예약하시면서 한 번 더 확인해 주세요.',\n",
       " '',\n",
       " '',\n",
       " '아침 식사는 무료예요. 메뉴판을 보시고 주문하시면 됩니다. 무제한으로 가능하세요. 내부 식당에서는 점심, 저녁 식사도 서비스하는데요, 베트남 평균 물가와 비교하면 살짝 비싸지만 사파 최고의 셰프라고 자신합니다. 아무거나 주문해도 다 맛있어요. 추천 메뉴는 돼지고기 요리와 로컬 와인! 역대급 감동을 보장합니다.',\n",
       " '',\n",
       " '',\n",
       " '누군가는 사파를 리틀 스위스라고 부른다네요. 아침에 방갈로에서 눈을 뜨면 실제로 보실 수 있는 전망입니다. 어떠세요? 정원과 산, 강 등 세 가지 옵션 중에서 전망을 선택하실 수 있는데요, 하나를 고르기 어려울 정도로 모두 추천합니다.',\n",
       " '',\n",
       " '',\n",
       " '방갈로는 항상 예약이 꽉 차 있어요. 놓쳤다고 해도 걱정하지 마시와요. 테라스가 없는 방도 창문이 크게 나 있어요. 중앙 마당 의자에만 앉아도 환상적인 뷰를 즐기실 수 있습니다. 밤에 벤치에 앉아 멍하니 하늘을 보면, 머리 위를 가득 수놓은 별도 발견하실 수 있어요.',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '하노이 공항에서부터는 셔틀을 운영하고 있어요. 사파 시내에서 오신다면 택시를 이용하셔야 해요. 숙소는 국립 유산 지역에 자리 잡고 있어서 들어오실 때 입장권(1인당 VND 75,000)구입은 필수! 티켓은 체크아웃 할 때까지 보관하세요.',\n",
       " '',\n",
       " '',\n",
       " '아무래도 자연 한가운데 위치해 벌레로부터 완전히 자유로울 수는 없겠죠? 특히 모기! 퇴치용품을 따로 준비해주시면 좋아요. 물론 숙소에서 모기장도 쳐주고 퇴치용으로 아로마 향도 골고루 뿌려주세요.',\n",
       " '',\n",
       " '',\n",
       " '2박 이상 머무르신다면 숙소에서 제공하는 액티비티에도 참여해보세요.',\n",
       " '라오차이 타반마을 트레킹, 쿠킹 클래스 등을 다양하게 운영 중입니다. 숙소 예약 시 일정 확인은 필수겠죠?',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '에코 팜스 하우스 – 사파 리트리트\\xa0살펴보기',\n",
       " '',\n",
       " '위치',\n",
       " '',\n",
       " 'Eco Palms House - Sapa Retreat',\n",
       " 'TT152, Lao Chải, Sa Pa, Lào Cai, 베트남',\n",
       " '',\n",
       " '',\n",
       " '올스테이 하고 싶은 이유',\n",
       " '인생 뷰 맛집',\n",
       " '사파 최고의 셰프',\n",
       " '액티비티',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '베트남 사파, 다른 숙소 가격 살펴보기',\n",
       " '',\n",
       " '',\n",
       " 'Photograph by @아고다 @에코 팜스 하우스 – 사파 리트리트 공식 홈페이지\\xa0\\xa0',\n",
       " '여행을 다루는 척 서비스를 홍보합니다. https://allstay.kr']"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_dict['지구한바퀴_세계여행?q=g']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['지구한바퀴_세계여행?q=g', '그림·웹툰?q=g'])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_dict.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
