{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "## url pickle load\n",
    "pickles = ['감성_에세이?q=g']\n",
    "\n",
    "\n",
    "writer_list = []\n",
    "for file in pickles:\n",
    "    with open(file[:-4]+\"_userId.txt\",\"rb\") as fr:\n",
    "        writers = pickle.load(fr)\n",
    "    writer_list.append(writers)  ## [[카테고리1 게시글 url...],[카테고리2 게시글 url], ....[카테고리24 게시글 url]]\n",
    "\n",
    "## 게시글 속 정보 수집\n",
    "def def_craw(writer):\n",
    "    \n",
    "    json_data = {}\n",
    "    data = []\n",
    "    res_text = []\n",
    "    tag_keyword=[]\n",
    "    \n",
    "    tag_title,tag_nickname,tag_publish_date,tag_url,tag_url_plink = None,None,None,None,None\n",
    "    tag_share,tag_like = str,str\n",
    "    for idx,url in enumerate(writer):\n",
    "        if idx % 500 == 0 : print(\"전체\",len(writer),\"중에\",idx)\n",
    "        if res_text == []: # 첫 시작에러 방지\n",
    "            pass\n",
    "        else :\n",
    "            # to json\n",
    "            json_data['title'] = tag_title  \n",
    "            json_data['nickname'] = tag_nickname\n",
    "            json_data['publish_date'] = tag_publish_date\n",
    "            json_data['keyword'] = tmp_keyword   \n",
    "            json_data['like'] = tag_like # like 없는 경우 ''\n",
    "            json_data['share'] = tag_share # share 없는 경우 None            \n",
    "            json_data['comment'] = tag_comment # comment 없는 경우 ''\n",
    "            json_data['url'] = tag_url\n",
    "            json_data['url_plink'] = tag_url_plink \n",
    "            json_data['text'] = res_text\n",
    "\n",
    "        data.append(json_data)\n",
    "        \n",
    "        json_data = {} # 누적방지 초기화\n",
    "        tmp_keyword = [] # 누적방지 초기화\n",
    "        res_text = [] # 누적방지 초기화 \n",
    "        \n",
    "        # beautifulsoup\n",
    "        html = requests.get('https://brunch.co.kr{text_url}'.format(text_url=url))\n",
    "        soup = BeautifulSoup(html.text, 'html.parser')\n",
    "        \n",
    "        if soup.find('title').text == \"brunch\":\n",
    "            pass\n",
    "        else:\n",
    "            tag_title = soup.find('title').text # 게시글 title\n",
    "            tag_url = soup.find(\"meta\",property='og:url')['content'] # 게시글 본주소\n",
    "            tag_nickname = soup.find(\"meta\",{'name':'article:media_name'})['content'] # 작가 nickname\n",
    "            tag_url_plink = soup.find(\"meta\",property='dg:plink')['content'] # 암호주소? # 모바일?\n",
    "            tag_publish_date = soup.find(\"meta\",property='article:published_time')['content'] # 발행일\n",
    "            tag_keyword = soup.find_all('a',href=re.compile('/keyword')) # 게시글 키워드\n",
    "            tag_like = soup.find('span',{'class':'f_l text_like_count text_default text_with_img_ico ico_likeit_like #like'}) #좋아요 수\n",
    "            tag_share = soup.find('span',{'class':'f_l text_share_count text_default text_with_img_ico'}) # 공유 수\n",
    "            tag_comment = soup.find('span',{'class':'f_l text_comment_count text_default text_with_img_ico'}) # 댓글 수\n",
    "            text_h4 = soup.find_all(class_='wrap_item item_type_text')\n",
    "            \n",
    "            for text in text_h4:\n",
    "                res_text.append(text.text)\n",
    "    \n",
    "            if tag_like == None:\n",
    "                tag_like = \"0\"\n",
    "            else:\n",
    "                tag_like = tag_like.text # 좋아요 수\n",
    "\n",
    "            if tag_share == None:\n",
    "                tag_share == \"0\"\n",
    "            else:\n",
    "                tag_share = tag_share.text # 공유 수\n",
    "\n",
    "            if tag_comment == None:\n",
    "                tag_comment ==\"0\"\n",
    "            else:\n",
    "                tag_comment = tag_comment.text\n",
    "\n",
    "            for keyword in tag_keyword:\n",
    "                tmp_keyword.append(keyword.text)\n",
    "                \n",
    "    return data ## 수집한 정보를 담은 dictionary로 반환\n",
    "\n",
    "categories = ['지구한바퀴_세계여행','시사_이슈','IT_트렌드',\n",
    "                '취항저격_영화리뷰','오늘은_이런책','뮤직_인사이드',\n",
    "                 '직장인_현실조언','스타트업_경험담','육아_이야기',\n",
    "                 '요리_레시피','건강_운동','멘탈관리_심리탐구',\n",
    "                 '문화_예술','인문학_철학','쉽게_읽는_역사',\n",
    "                '우리집_반려동물','사랑_이별','감성_에세이']\n",
    "\n",
    "\n",
    "## 카테고리 -> 게시글의 순서로 2차 크롤링 진행\n",
    "## 카테고리별로 정보를 담은 json 형식으로 저장(총 24개 json file)\n",
    "from collections import OrderedDict\n",
    "for idx,writer in enumerate(writer_list):\n",
    "    to_json = None\n",
    "    data = def_craw(writer) # 2단계 크롤링 실행\n",
    "    \n",
    "    del data[0]\n",
    "    del data[0]\n",
    "    \n",
    "    to_json = OrderedDict()\n",
    "    to_json['name'] = categories[idx] # category name\n",
    "    to_json['version'] = \"2020-06-01\"\n",
    "    to_json['data'] = data\n",
    "    \n",
    "    with open(categories[idx]+\".json\",\"w\") as make_file:\n",
    "        json.dump(to_json,make_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'낯선 곳에서 맞이하는 아침은 언제나 상쾌하다 새벽 늦게 수다를 떨다 잠들어도 밤새 술을 마시다 잠들어도 집 침대가 아닌 곳에서 일어나는 아침은 마치 영화처럼 지저귀는 새소리와 함께 단번에 침대에서 나올 이 생긴다 쳇바퀴 굴리는 삶이라 일어나면 어제와 같은 하루가 펼쳐질 것이라는 것을 아는 권태에 가득 찬 아침 침대에서 분만 분 만을 중얼거리는 것과는 딴 판이다 눈을 뜨면 무슨 하루가 펼쳐질까 궁금하니 일어나는 것도 즐거웠다 눈을 뜨고 씻기도 전에 게스트 하우스의 식당으로 가서 짜이 한 잔을 마셨다 아침 일찍 일어나 피곤에 찌들어 커피를 한 잔 마시며 잠에서 깨는 것이 일상이었으나 인도에서는 이 짜이라는 차가 커피를 대신하게 되었다 인도에 왔으면 인도의 룰을 따라야 했다 짜이 한 잔과 함께 게스트하우스를 오가는 사람들에게 눈인사를 나누는 동안 모스크의 아침 종소리가 맨발을 통해 진동으로 울려 퍼진다 대충 차려 입고 바라나시를 구경하러 밖으로 나왔다 무너질 듯 아슬아슬한 회색 골목을 지나면 화려한 색상들의 상점들이 분주하다 아침부터 요란스러운 오토바이 소리와 하품하는 개 아직 쓰레기 더미 사이에서 잠이 든 소를 지나치면 이 골목 사랑방인 짜이 집이 나왔다 오늘도 역시 많은 인도인들이 이 짜이 집 앞에서 황토색 컵에 찰랑거리는 짜이를 담고 수다를 떨고 있다 계급 지위 민족을 막론하고 인도인들을 하나로 묶어주는 것은 바로 이 짜이다 우리나라 사람들을 하나로 묶어 주는 것은 무엇일까 한국이라는 문화 DNA가 모든 한국인에게 발현되어 하나로 묶어주는 것은 다름이 아니라 김치일 것이다 경상도 사람이든 전라도 사람이든 혹은 이북에 살든 한국 문화의 DNA가 하나로 통하는 것은 김치다 다른 사람들은 우리를 하나로 묶는 문화 DNA는 아리랑이나 한 혹은 정 이라고 말한다 하지만 아리랑을 우리를 묶어준다고 하기에는 현대 한국을 사는 사람들에겐 큰 감흥이 오지 않는다 낯설지는 않지만 그렇다고 평소에도 즐겨 들으며 아리랑을 부르지는 않는다 한은 추상적이다 우리가 직접 체감하기 어려운 부분이 있다 정은 우리나라뿐만 아니라 어느 나라에서나 보편적인 인류애로 만나볼 수 있다 하지만 김치는 우리가 평소에도 쉽게 접하며 우리의 문화 안에 깊숙하게 뿌리내리고 있으며 체험하기도 쉽다 그리고 우리와 같은 문화를 공유한 사람이라면 많은 사람들이 인정하는 우리 삶에서 가장 중요한 음식이다 지역별로 만드는 방법이나 들어가는 재료는 달라도 김치는 우리 문화 DNA에 가장 크게 각인된 것이다 여기서 문화 DNA라는 것은 우리 민족에만 국한된 것이 아니다 한민족이라는 핏줄을 공유하지 않았어도 우리 문화권에서 자라나며 친숙해진 외국인이나 이민자 혹은 기타 다양한 사람들도 우리의 문화 DNA는 공유하게 되고 그들 또한 김치에 익숙해진다 이러한 모습을 문화 DNA에 물드는 것이 아니라 문화 동화주의로 우리의 문화를 외국인에게 강조한다는 말은 우선은 제쳐 두고 생각하자 결국 우리나라는 동일 문화라는 큰 테두리 안에서 오랫동안 유지되어 온 국가이기 때문에 같은 문화 DNA를 공유하는 방향으로 나아갔기 때문이다 결론적으로 김치는 우리 문화에서 가장 대표적이고 상징적인 요소이며 우리 문화 DNA에 가장 크게 각인된 것이다 한동안 우리나라가 한식을 홍보한다며 김치를 해외 광고에 올리고 한 것은 결국 우리나라에 가장 큰 부분을 차지하는 문화 DNA를 강조하고 싶은 것이 아니었을까 싶다 우리 문화에서 김치가 차지하는 비중만큼 인도에선 짜이가 중요하다 언제나 어디서나 누구나 짜이를 만나고 있기 때문이다 아무리 가난한 사람이라도 쉬는 시간에는 꼭 짜이를 마셔야 하는 인도인이다 부자든 거지든 일단 하루에 한 잔은 짜이를 마셔야 한다 남인도에 살든 북인도에 살든 인도인이면 짜이를 마신다 인도에서 짜이는 영국식 밀크티에 향신료가 첨가된 인도식 차인 마살라 짜이를 의미한다 보통의 영국식 밀크티에 다양한 향신료와 설탕을 듬뿍 넣어 만든 짜이는 우리가 아는 일반적인 차와는 맛이 달라 독특하다 한국에서 만나는 차는 보통 청아하게 우려내 푸른 향내를 즐기는 맑은 차이다 하지만 이 짜이는 우유의 부드러움과 깊은 맛에 각종 향신료의 다양한 향 그리고 숨어있는 은은한 차 향기 덕분에 마치 차가 아닌 마시는 음식을 만나는 기분이다 김치의 맛이 집집마다 각양각색이듯 짜이도 짜이를 만드는 사람마다 집어넣는 향신료의 양이 다르기 때문에 맛이 확연히 차이가 난다 비싼 찻잎을 많이 넣는 고급 레스토랑에서 마시는 짜이는 고급 머그컵에 담겨 나와 풍부한 찻잎의 향이 입 안에 가득 찬다 반면 길거리에서 쉽게 만나는 저렴한 짜이는 찻잎보다는 향신료가 더 첨가되어 생강 맛이 더 진하거나 혹은 계피의 향이 물씬 풍긴다 그리고 이러한 길거리 짜이는 종이컵이나 황토 흙으로 만든 잔에 담겨 나온다 하지만 고급 레스토랑이던 길거리에서 파는 짜이던 인도인이라면 하루에 수 잔의 짜이는 반드시 필요하다 낯선 거리를 지나가는 우리를 본 짜이 집 할아버지는 우리에게 한 잔 하라는 손짓을 한다 한잔 하고 가라는 유혹이다 주변에 서서 짜이를 마시던 손님들도 우리를 보고 웃는다 황토 흙으로 만든 컵에 연한 밀크 티 색의 짜이가 찰랑거렸다 한 잔에 단 돈 루피 우리 돈으로 원 자판기 커피보다 저렴한 가격이었다 할아버지가 만든 짜이에는 생강이 많이 들어가 톡 쏘는 생강 맛이 진하게 퍼졌다 짜이를 마시던 손님들 중 몇 명은 우리에게 관심이 생겼는지 다가와 다양한 질문 공세를 퍼붓고 같이 사진을 찍어도 되냐는 말을 했다 많은 사람들이 드나들지 않는 골목 중의 골목이라 이런 곳에 머물던 한국인이 신기한 모양이었다 짜이는 낯선 도시를 걷는 외국인조차 다가가게 해 줄 뿐만 아니라 인도인들의 강한 문화 DNA이자 삶의 일부였다'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = requests.get('https://brunch.co.kr/@pubss/175')\n",
    "soup = BeautifulSoup(html.text, 'html.parser')\n",
    "\n",
    "text_h4 = soup.find_all(class_='wrap_item item_type_text')\n",
    "res_text=[]\n",
    "for text in text_h4:\n",
    "    res_text.append(text.text)\n",
    "    \n",
    "str(res_text)\n",
    "\n",
    "import re\n",
    "def pre_text_2(x):\n",
    "    pa = re.compile(\"^\\\\\\\\xa0|xa\") # xla 등 불용어 제거\n",
    "    pa1 = re.compile(r\"'http.*?'\") # 전체 url 제거\n",
    "    pa2 = re.compile(r'\\([^)]*\\)') # () 사이 문자 \n",
    "    pa3 = re.compile('[^\\w\\s]') # 특수문자 삭제\n",
    "    pa4 = re.compile(r'[^a-zA-Zㄱ-힗]') # 한글,영어만 남김\n",
    "\n",
    "    x = re.sub(pa,' ',x)\n",
    "    x = re.sub(pa1,' ',x)\n",
    "    x = re.sub(pa2,' ',x)\n",
    "    x = re.sub(pa3, ' ',x)\n",
    "    x = re.sub(pa4, ' ',x)\n",
    "    x = x.strip()\n",
    "    x = \" \".join(x.split())\n",
    "    return x \n",
    "\n",
    "pre_text_2(str(res_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
