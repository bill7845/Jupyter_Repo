{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#raw data loading\n",
    "df_it = pd.read_csv('Article_IT과학_201906_201908.csv',encoding='euc-kr',names=['날짜','분야','신문사','제목','본문','URL'])\n",
    "df_politic = pd.read_csv('Article_정치_201906_201908.csv',encoding='euc-kr',names=['날짜','분야','신문사','제목','본문','URL'])\n",
    "df_economy = pd.read_csv('Article_경제_201906_201908.csv',encoding='euc-kr',names=['날짜','분야','신문사','제목','본문','URL'])\n",
    "df_life = pd.read_csv('Article_생활문화_201906_201908.csv',encoding='euc-kr',names=['날짜','분야','신문사','제목','본문','URL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복기사 제거\n",
    "df_it = df_it.drop_duplicates(['신문사', '본문'], keep='first')\n",
    "df_politic = df_politic.drop_duplicates(['신문사', '본문'], keep='first')\n",
    "df_economy = df_economy.drop_duplicates(['신문사', '본문'], keep='first')\n",
    "df_life = df_life.drop_duplicates(['신문사', '본문'], keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리\n",
    "\n",
    "### 기사 내용에서 명사추출\n",
    "\n",
    "### 추출한 명사로 Dictionary 생성\n",
    "\n",
    "### 원핫인코딩 or 임베딩 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_it['본문'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from multiprocessing import Process, Queue\n",
    "import getN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [df_it['본문'], df_politic['본문'], df_economy['본문'], df_life['본문']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4개 category 한번에 명사추출하려고 multiprocessing 한것\n",
    "if __name__ == '__main__':\n",
    "    q = Queue()    \n",
    "    for cate in data:\n",
    "        #sample 수는 알아서 조절\n",
    "        sample = cate[0:10000]\n",
    "        proc = Process(target=getN.getnouns, args=(sample, q))\n",
    "        proc.start()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 명사 추출한게 queue에 들어있는데 이걸 result 리스트에 저장\n",
    "result = []\n",
    "while q.qsize() != 0:\n",
    "    result.append(q.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원래 data의 첫번째 기사 읽고 result의 [0]번째 결과랑 확인 해보면서 category 찾으세요\n",
    "data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[3][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizer\n",
    "import json\n",
    "#json파일 읽어서 tokenizer로 사용\n",
    "f = open('5000.json','r')\n",
    "jsondata = f.read()\n",
    "all = json.loads(jsondata)\n",
    "# 5000개도 해보고 10000개도 해보고\n",
    "tokenizer_5 =Tokenizer(num_words=5000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_5.word_index = json.loads(all[\"config\"][\"word_index\"])\n",
    "tokenizer_5.index_word = json.loads(all[\"config\"][\"index_word\"])\n",
    "tokenizer_5.word_counts = json.loads(all[\"config\"][\"word_counts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result 리스트 결과 ex) 0: 경제 , 1: 정치 2: 생활 3: IT\n",
    "ec_result = tokenizer_5.texts_to_matrix(result[0], mode='binary')\n",
    "po_result = tokenizer_5.texts_to_matrix(result[1], mode='binary')\n",
    "li_result = tokenizer_5.texts_to_matrix(result[2], mode='binary')\n",
    "it_result = tokenizer_5.texts_to_matrix(result[3], mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "sourcedata = [[0],[1],[2],[3]]\n",
    "enc.fit(sourcedata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ec_y = np.zeros((10000,1))\n",
    "po_y = np.zeros((10000,1)) + 1\n",
    "li_y = np.zeros((10000,1)) + 2\n",
    "it_y = np.zeros((10000,1)) + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = np.concatenate((ec_result,po_result,li_result,it_result), axis=0)\n",
    "y_total = np.concatenate((ec_y,po_y,li_y,it_y), axis=0)\n",
    "total = np.concatenate((total, y_total), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "data_train, data_test, label_train, label_test = train_test_split(total[:,:-1],total[:,-1])\n",
    "label_train = enc.transform(label_train.reshape(30000,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models, layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(5000,)))\n",
    "model.add(layers.Dense(4, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(data_train, label_train, epochs=20, batch_size=512, validation_split=0.2, shuffle=True)\n",
    "history_dict = history.history\n",
    "history_dict.keys()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "acc = history.history['accuracy']#TODO\n",
    "val_acc = history.history['val_accuracy']#TODO\n",
    "loss = history.history['loss']#TODO\n",
    "val_loss = history.history['val_loss']#TODO\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')# ‘bo’는 파란색 점\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')# ‘b’는 파란색 실선\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.clf()   # 그래프를 초기화\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
