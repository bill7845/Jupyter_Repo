{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "각 단어의 인덱스: \n",
      " {'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5, 'dog': 6, 'ate': 7, 'my': 8, 'homework': 9}\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "samples = ['The cat sat on the mat ', 'The dog ate my homework.']\n",
    "\n",
    "\n",
    "# 가장 빈도가 높은 1,000개의 단어만 선택하도록 Tokenizer 객체 생성\n",
    "tokenizer =Tokenizer(num_words=1000)\n",
    "# 단어 인덱스를 구축\n",
    "tokenizer.fit_on_texts(samples)  \n",
    "word_index = tokenizer.word_index  \n",
    "\n",
    "print(\"각 단어의 인덱스: \\n\", word_index)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer정보를 json 파일로 저장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"test.json\",\"w\")    \n",
    "f.write( tokenizer.to_json()  )\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer정보를 json 파일 load  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체  {'class_name': 'Tokenizer', 'config': {'num_words': 1000, 'filters': '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', 'lower': True, 'split': ' ', 'char_level': False, 'oov_token': None, 'document_count': 2, 'word_counts': '{\"the\": 3, \"cat\": 1, \"sat\": 1, \"on\": 1, \"mat\": 1, \"dog\": 1, \"ate\": 1, \"my\": 1, \"homework\": 1}', 'word_docs': '{\"on\": 1, \"mat\": 1, \"sat\": 1, \"the\": 2, \"cat\": 1, \"homework\": 1, \"ate\": 1, \"dog\": 1, \"my\": 1}', 'index_docs': '{\"4\": 1, \"5\": 1, \"3\": 1, \"1\": 2, \"2\": 1, \"9\": 1, \"7\": 1, \"6\": 1, \"8\": 1}', 'index_word': '{\"1\": \"the\", \"2\": \"cat\", \"3\": \"sat\", \"4\": \"on\", \"5\": \"mat\", \"6\": \"dog\", \"7\": \"ate\", \"8\": \"my\", \"9\": \"homework\"}', 'word_index': '{\"the\": 1, \"cat\": 2, \"sat\": 3, \"on\": 4, \"mat\": 5, \"dog\": 6, \"ate\": 7, \"my\": 8, \"homework\": 9}'}}\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "[[1, 2, 3, 4, 1, 5], [1, 6, 7, 8, 9]]\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "f2 = open(\"test.json\",\"r\") \n",
    "jsondata = f2.read()\n",
    "all = json.loads(jsondata)\n",
    "print (\"전체 \", all)\n",
    "\n",
    "\n",
    "tokenizer2 = Tokenizer(num_words=1000) \n",
    " \n",
    "tokenizer2.word_index = json.loads(all[\"config\"][\"word_index\"]  )\n",
    "tokenizer2.index_word = json.loads(all[\"config\"][\"index_word\"]   )\n",
    "tokenizer2.word_counts = json.loads(all[\"config\"][\"word_counts\"]  ) \n",
    " \n",
    "print(tokenizer2.texts_to_sequences(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
