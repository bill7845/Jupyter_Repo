{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words : 4024\n",
      "0.6950000000000001\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "##########################################################################################################\n",
    "\n",
    "def get_file_list(dir_name): # file name들을 가져오는 함수 # 폴더명 인자 # 폴더가 위치한 경로를 인자로\n",
    "    return os.listdir(dir_name) # 폴더 내 파일명을 리스트 형태로 반환 \n",
    "\n",
    "##########################################################################################################\n",
    "\n",
    "# 현재 8명의 선수에 대해 각각 10개씩의 기사 (야구4,축구4)\n",
    "# 기사 카테고리의 경우 선수별로 1_~~~.txt 형식으로 맨 앞 숫자로 누구의 기사인지 구분\n",
    "\n",
    "def get_conetents(file_list): # 모든 파일의 상대경로 리스트를 인자로\n",
    "    y_class = []\n",
    "    X_text = []\n",
    "    class_dict = { # 야구선수와 축구선수에 대해 0,1의 클래스로 분류할것\n",
    "        1: \"0\", 2: \"0\", 3:\"0\", 4:\"0\", 5:\"1\", 6:\"1\", 7:\"1\", 8:\"1\"} # 파일명의 맨 앞 숫자로 분류\n",
    "\n",
    "    for file_name in file_list: \n",
    "        try:\n",
    "            f = open(file_name, \"r\",  encoding=\"cp949\") # winodw => cp949, 파일 읽기\n",
    "            category = int(file_name.split(os.sep)[1].split(\"_\")[0]) # 파일명 맨앞 숫자 빼내어 int형으로 \n",
    "            # os.sep => \\,/\n",
    "            y_class.append(class_dict[category]) # 해당파일이 0,1의 클래스 중 어디에 속하는지 (class_dict에 맞추어 분류됨)\n",
    "            X_text.append(f.read()) # 해당파일의 text \n",
    "            f.close()\n",
    "        except UnicodeDecodeError as e:\n",
    "            print(e)\n",
    "            print(file_name)\n",
    "    return X_text, y_class # 모든 파일들에 대해 각각의 text와 종목클래스 정보 반환\n",
    "\n",
    "\n",
    "\n",
    "def get_cleaned_text(words): #\n",
    "    import re\n",
    "    words = re.sub('\\W+','', words.lower() ) # 각각의 단어들을 소문자로 바꾸고, 문장부호를 없에고 반환\n",
    "    return words\n",
    "\n",
    "\n",
    "def get_corpus_dict(text): # 모든 text를 인자로 받음(80개)\n",
    "    text = [sentence.split() for sentence in text] # 각 글별로 split을 진행하여 text라는 리스트에 저장\n",
    "    # split()은 결과물을 리스트로 반환함\n",
    "    # 2차원 리스트로 저장됨. # [[글1의 단어들],[글2의 단어들],[글3의 단어들]....]\n",
    "    cleand_words = [get_cleaned_text(word) for words in text for word in words] \n",
    "    # [글1의 단어들]에서 단어들을 하나씩 뽑은 후 다시 글자 하나 단위로 뽑아 get_cleaned함수를 적용\n",
    "    \n",
    "    # list comprehension에서 for문이 나란히 두개인경우 앞에 for문부터 실행. 아래와 같다\n",
    "    # for words in text:\n",
    "    #     for word in words:\n",
    "    #         get_cleaned_text(word)\n",
    "\n",
    "    from collections import OrderedDict # dict의 값을 순서대로 사용가능. # 원래 dict는 순서 없음.\n",
    "    corpus_dict = OrderedDict()\n",
    "    for i, v in enumerate(set(cleand_words)): # 중복제거하여 단어사전을 만듬\n",
    "        corpus_dict[v] = i # 각 단어별로 index를 지정\n",
    "    return corpus_dict # 각 단어를 key값으로 갖는 dict를 반환\n",
    "\n",
    "\n",
    "def get_count_vector(text, corpus): # 문서별 단어빈도를 vector로 \n",
    "    text = [sentence.split() for sentence in text]\n",
    "    word_number_list = [[corpus[get_cleaned_text(word)] for word in words] for words in text]\n",
    "    # get_corpus_dict와 같은 원리이지만 2차원 리스트로 반환함. # 각 문서별로 구분하기위해\n",
    "    X_vector = [[0 for _ in range(len(corpus))] for x in range(len(text))]\n",
    "    # 80 4030? 의 matrix 생성 # _ => 변수를 사용하지 않는다.(0으로 행렬을 채워넣는다)\n",
    "\n",
    "    for i, text in enumerate(word_number_list): # 각 문서별로 corpus_dict에 맞추어 빈도수 구해줌\n",
    "        for word_number in text:\n",
    "            X_vector[i][word_number] += 1\n",
    "    return X_vector # 문서별 단어빈도수까지 추가\n",
    "\n",
    "import math\n",
    "def get_cosine_similarity(v1,v2): # 문서 2개의 벡터를 넣어 코사인 유사도 계산\n",
    "    \"compute cosine similarity of v1 to v2: (v1 dot v2)/{||v1||*||v2||)\"\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    return sumxy/math.sqrt(sumxx*sumyy)\n",
    " \n",
    "def get_similarity_score(X_vector, source): # X_vector => 전체(80개)문서 벡터,source => target text vector\n",
    "    source_vector = X_vector[source] # target text를 몇번째 문서로 할것인가\n",
    "    similarity_list = []\n",
    "    for target_vector in X_vector:\n",
    "        similarity_list.append(get_cosine_similarity(source_vector, target_vector))\n",
    "        # ex) 10번째 문서와 나머지 문서간의 코사인 유사도를 계산하여 리스트형태로 반환\n",
    "        # 여기서는 1개와 나머지 80개(자기자신포함)의 유사도 계산 => 자기자신과의 코사인 유사도는 1 \n",
    "    return similarity_list\n",
    "\n",
    "\n",
    "def get_top_n_similarity_news(similarity_score, n): # 가장 높은 유사도를 가진 문서 n개 \n",
    "    import operator\n",
    "    x = {i:v for i, v in enumerate(similarity_score)}\n",
    "    sorted_x = sorted(x.items(), key=operator.itemgetter(1))\n",
    "\n",
    "    return list(reversed(sorted_x))[1:n+1] # 유사도 1은 제외\n",
    "\n",
    "def get_accuracy(similarity_list, y_class, source_news): \n",
    "    source_class = y_class[source_news]\n",
    "#   target과 유사도가 가장 높은 n개의 문서들의 클래스를 이용하여 정확도 계산\n",
    "#   ex) 10개중 0이 8개면 80% \n",
    "    return sum([source_class == y_class[i[0]] for i in similarity_list]) / len(similarity_list)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dir_name = \"C:/Users/KIHyuk/Desktop/부스트코스/AI-python-connect-master/codes/ch_1/news/news_data\" # 폴더명 * 폴더위치\n",
    "    file_list = get_file_list(dir_name) # 폴더 내에 파일명을 리스트로 return\n",
    "    file_list = [os.path.join(dir_name, file_name) for file_name in file_list] # 파일의 상대경로까지\n",
    "    # os.path => 각 os방식에 맞추어 경로 연결 (window => \\, mac => /)\n",
    "\n",
    "    X_text, y_class = get_conetents(file_list)\n",
    "#   X_text => 모든 text 리스트\n",
    "#   y_class => 각 text 별 0,1 클래스\n",
    "\n",
    "    corpus = get_corpus_dict(X_text)\n",
    "    print(\"Number of words : {0}\".format(len(corpus)))\n",
    "    X_vector = get_count_vector(X_text, corpus)\n",
    "    source_number = 10\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for i in range(80): \n",
    "        source_number = i # 각 기사별로 나머지 기사와의 유사도 계산하여 \n",
    "\n",
    "        similarity_score = get_similarity_score(X_vector, source_number)\n",
    "        similarity_news = get_top_n_similarity_news(similarity_score, 10)\n",
    "        accuracy_score = get_accuracy(similarity_news, y_class, source_number)\n",
    "        result.append(accuracy_score)\n",
    "    print(sum(result) / 80) # 전체 평균 정확도"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
