{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tf.keras.models import Sequential # This does not work!\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Input, Dense, GRU, Embedding\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import font_manager, rc\n",
    "import matplotlib\n",
    "\n",
    "#한글 폰트 등록\n",
    "font_location = \"c:/Windows/fonts/malgun.ttf\"\n",
    "font_name = font_manager.FontProperties(fname=font_location).get_name()\n",
    "matplotlib.rc('font', family=font_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # 판다스 데이터프레임(DataFrame)을 출력할 때, 최대 출력할 수 있는 컬럼을 100개로 늘려줍니다.\n",
    "# # 이렇게 해야 데이터를 분석할 때 출력해서 확인하기 편합니다.\n",
    "pd.options.display.max_columns = 500\n",
    "pd.options.display.max_rows = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv('C:/Users/KIHyuk/Desktop/전력수요예측/train.csv')\n",
    "# test = pd.read_csv('C:/Users/KIHyuk/Desktop/전력수요예측/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_time = pd.read_csv('C:/Users/KIHyuk/Desktop/전력수요예측/인천_시간별_기상자료.csv',encoding='euc-kr',index_col='일시',parse_dates=True)\n",
    "del weather_time['지점']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in range(1,len(train.columns) ): #시간을 제외한 1열부터 마지막 열까지를 for문으로 작동시킵니다.\n",
    "#     train_median=train.iloc[:,k].median() #값을 대체하는 과정에서 값이 변경 될 것을 대비해 해당 세대의 중앙값을 미리 계산하고 시작합니다.\n",
    "#     counting=train.loc[ train.iloc[:,k].isnull()==False ][ train.columns[k] ].index\n",
    "\n",
    "#     df=pd.DataFrame( list( zip( counting[:-1], counting[1:] - counting[:-1] -1  ) ), columns=['index','count'] )\n",
    "    \n",
    "#     df2= df[ (df['count'] > 0) ] #결측치가 존재하는 부분만 추출\n",
    "#     df2=df2.reset_index(drop=True) #기존에 존재하는 index를 초기화 하여 이후 for문에 사용함\n",
    "\n",
    "#     for i,j in zip( df2['index'], df2['count'] ) : # i = 해당 세대에서 값이 존재하는 index, j = 현재 index 밑의 결측치 갯수\n",
    "#         if train.iloc[i,k]>=train_median: #현재 index에 존재하는 값이 해당 세대의 중앙 값 이상일때만 분산처리 실행\n",
    "#             train.iloc[ i : i+j+1 , k] = train.iloc[i,k] / (j+1) \n",
    "#             #현재 index 및 결측치의 갯수 만큼 지정을 하여, 현재 index에 있는 값을 해당 갯수만큼 나누어 줍니다\n",
    "#         else:\n",
    "#             pass\n",
    "#             #현재 index에 존재하는 값이 중앙 값 미만이면 pass를 실행\n",
    "#     if k%50==0: #for문 진행정도 확인용\n",
    "#             print(k,\"번째 실행중\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in range(1,len(test.columns) ): #시간을 제외한 1열부터 마지막 열까지를 for문으로 작동시킵니다.\n",
    "#     test_median=test.iloc[:,k].median() #값을 대체하는 과정에서 값이 변경 될 것을 대비해 해당 세대의 중앙값을 미리 계산하고 시작합니다.\n",
    "#     counting=test.loc[ test.iloc[:,k].isnull()==False ][ test.columns[k] ].index\n",
    "\n",
    "#     df=pd.DataFrame( list( zip( counting[:-1], counting[1:] - counting[:-1] -1  ) ), columns=['index','count'] )\n",
    "    \n",
    "#     df2= df[ (df['count'] > 0) ] #결측치가 존재하는 부분만 추출\n",
    "#     df2=df2.reset_index(drop=True) #기존에 존재하는 index를 초기화 하여 이후 for문에 사용함\n",
    "\n",
    "#     for i,j in zip( df2['index'], df2['count'] ) : # i = 해당 세대에서 값이 존재하는 index, j = 현재 index 밑의 결측치 갯수\n",
    "#         if test.iloc[i,k]>=test_median: #현재 indetestx에 존재하는 값이 해당 세대의 중앙 값 이상일때만 분산처리 실행\n",
    "#             test.iloc[ i : i+j+1 , k] = test.iloc[i,k] / (j+1) \n",
    "#             #현재 index 및 결측치의 갯수 만큼 지정을 하여, 현재 index에 있는 값을 해당 갯수만큼 나누어 줍니다\n",
    "#         else:\n",
    "#             pass\n",
    "#             #현재 index에 존재하는 값이 중앙 값 미만이면 pass를 실행\n",
    "#     if k%50==0: #for문 진행정도 확인용\n",
    "#             print(k,\"번째 실행중\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train['Time'] = train['Time'].apply(pd.to_datetime)\n",
    "# train.set_index('Time',inplace=True)\n",
    "\n",
    "# test['Time'] = test['Time'].apply(pd.to_datetime)\n",
    "# test.set_index('Time',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sample = train['2018-02-15':]\n",
    "# train_sample = train_sample.interpolate(method='values')\n",
    "\n",
    "# test_sample = test['2018-02-15':]\n",
    "# test_sample = test_sample.interpolate(method='values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 결측값 가진 행 제거 - 데이터가 아예 없는 경우\n",
    "# train_sample.loc[:,train_sample.isnull().sum() >= 1].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #결측값 가진 행 제거 - 데이터가 아예 없는 경우\n",
    "# test_sample.loc[:,test_sample.isnull().sum() >= 1].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # forecast_test.csv 생성시에는 여기 실행 X\n",
    "\n",
    "# train_sample.drop(['X4', 'X127', 'X9', 'X65', 'X54', 'X13', 'X53', 'X24', 'X17', 'X14',\n",
    "#        'X48', 'X2', 'X19', 'X36', 'X28', 'X63', 'X38', 'X39', 'X8', 'X64',\n",
    "#        'X29', 'X57', 'X705', 'X71', 'X3', 'X27', 'X22', 'X21', 'X118', 'X6',\n",
    "#        'X45', 'X40', 'X66', 'X52', 'X12', 'X49', 'X1', 'X33', 'X60', 'X46',\n",
    "#        'X963', 'X56', 'X15', 'X11', 'X25', 'X34', 'X23', 'X58'], axis=1, inplace=True)\n",
    "\n",
    "# test_sample.drop(['X26', 'X16', 'X7', 'X18', 'X41', 'X55', 'X5', 'X43', 'X59', 'X10',\n",
    "#        'X62', 'X61', 'X32', 'X31', 'X30', 'X51', 'X35', 'X44', 'X37', 'X42',\n",
    "#        'X50', 'X47', 'X20'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################# 결측치 처리 ###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1차 중간저장 ( 결측치 처리)\n",
    "\n",
    "# train_sample.to_csv('train_sample')\n",
    "# test_sample.to_csv('test_sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 결측치 처리 된 Data Load\n",
    "train_sample = pd.read_csv('train_sample',index_col='Time',parse_dates=True)\n",
    "test_sample = pd.read_csv('test_sample',index_col='Time',parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = pd.concat([train_sample,test_sample],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################# 입력신호 추가 ###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total['temp'] = weather_time['기온(°C)']['2018-02-15':]\n",
    "total['humidity'] = weather_time['습도(%)']['2018-02-15':]\n",
    "total[['temp','humidity']] = total[['temp','humidity']].interpolate(method='values')  # 온도 결측값을 시간에 따른 보간법으로 채움\n",
    "total['DayOfWeek'] = train_sample.index.dayofweek\n",
    "total['Hour'] = train_sample.index.hour\n",
    "\n",
    "test_sample['temp'] = weather_time['기온(°C)']['2018-02-15':]\n",
    "test_sample['humidity'] = weather_time['습도(%)']['2018-02-15':]\n",
    "test_sample[['temp','humidity']] = test_sample[['temp','humidity']].interpolate(method='values')  # 온도 결측값을 시간에 따른 보간법으로 채움\n",
    "test_sample['DayOfWeek'] = train_sample.index.dayofweek\n",
    "test_sample['Hour'] = test_sample.index.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast_test csv 생성\n",
    "# test_sample.to_csv('forecast_test_시간별') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## holiday_list 에 공휴일과 토요일/일요일 날짜를 넣음\n",
    "\n",
    "# 공휴일 목록\n",
    "holiday_list=['2018-02-15','2018-02-16','2018-02-17','2018-03-01','2018-05-05','2018-05-07','2018-05-22','2018-06-06','2018-06-13']\n",
    "\n",
    "# 토요일/일요일 \n",
    "\n",
    "a= train_sample['2018-02-17':].index\n",
    "b= train_sample['2018-02-19':].resample('B').sum().index\n",
    "\n",
    "list_a = []\n",
    "for i in range(len(a.values)):\n",
    "    list_a.append(a.values[i])\n",
    "list_b = []\n",
    "for i in range(len(b.values)):\n",
    "    list_b.append(b.values[i])\n",
    "\n",
    "list_tmp = []\n",
    "for i in list_a:\n",
    "    if i not in list_b:\n",
    "        list_tmp.append(i)\n",
    "      \n",
    "    \n",
    "# list_tmp # 토요일/일요일\n",
    "saturday_sunday = np.array(list_tmp) # 툐요일/일요일 index => ndarray\n",
    "\n",
    "# # 주말목록 / 주말색인 구한 뒤 합침\n",
    "for i in range(len(saturday_sunday)):\n",
    "    holiday_list.append(str(saturday_sunday[i])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:323: DataConversionWarning: Data with input dtype int64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:323: DataConversionWarning: Data with input dtype int64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "## 데이터 minmax scaling\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# train/test/temp/humidity/DayOfWeek 각 feature 별로 scaler 구현\n",
    "# sc_train = MinMaxScaler() # trainSet scaler\n",
    "# sc_test = MinMaxScaler() # testSet scaler\n",
    "sc_MinMax = MinMaxScaler()\n",
    "sc_temp = MinMaxScaler() # temp scaler\n",
    "sc_humidity = MinMaxScaler() # humidity scaler\n",
    "sc_DayOfWeek = MinMaxScaler() # DayOfWeek scaler\n",
    "sc_Hour = MinMaxScaler()\n",
    "\n",
    "# scaling\n",
    "total_sc = sc_MinMax.fit_transform(total.iloc[:,:-4])\n",
    "test_sample_sc = sc_MinMax.fit_transform(test_sample.iloc[:,:-4])\n",
    "temp_sc = sc_temp.fit_transform(total[['temp']])\n",
    "humidity_sc = sc_humidity.fit_transform(total[['humidity']])\n",
    "DayOfWeek_sc = sc_DayOfWeek.fit_transform(total[['DayOfWeek']])\n",
    "Hour_sc = sc_Hour.fit_transform(total[['Hour']])\n",
    "\n",
    "# feature scale 데이터를 다시 합침 ( train + feature)\n",
    "# np.cocatenate\n",
    "total_sc = np.concatenate((total_sc, temp_sc), axis=1)\n",
    "total_sc = np.concatenate((total_sc, humidity_sc), axis=1)\n",
    "total_sc = np.concatenate((total_sc, DayOfWeek_sc), axis=1)\n",
    "total_sc = np.concatenate((total_sc, Hour_sc), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# feature scale 데이터를 다시 합침 ( test + feature)\n",
    "# np.concatenate\n",
    "test_sample_sc = np.concatenate((test_sample_sc, temp_sc), axis=1)\n",
    "test_sample_sc = np.concatenate((test_sample_sc, humidity_sc), axis=1)\n",
    "test_sample_sc = np.concatenate((test_sample_sc, DayOfWeek_sc), axis=1)\n",
    "test_sample_sc = np.concatenate((test_sample_sc, Hour_sc), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 다시 DataFrame 으로\n",
    "total = pd.DataFrame(total_sc, columns=total.columns,index=total.index)\n",
    "test_sample = pd.DataFrame(test_sample_sc, columns=test_sample.columns,index=test_sample.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# train,test DataFrame에 주말/공휴일 feature를 추가\n",
    "\n",
    "total['holiDay'] = 0 # holiday feature 추가 / 우선 0으로 \n",
    "\n",
    "for i in range(len(total)):\n",
    "    if str(total.index[i])[:10] in holiday_list :\n",
    "        total['holiDay'].iloc[i] = 1 # 쉬는날이면 1\n",
    "    else:\n",
    "        total['holiDay'].iloc[i] = 0 # 쉬는날 아니면 0\n",
    "        \n",
    "test_sample['holiDay'] = 0 # holiday feature 추가 / 우선 0으로 \n",
    "\n",
    "for i in range(len(test_sample)):\n",
    "    if str(test_sample.index[i])[:10] in holiday_list :\n",
    "        test_sample['holiDay'].iloc[i] = 1 # 쉬는날이면 1\n",
    "    else:\n",
    "        test_sample['holiDay'].iloc[i] = 0 # 쉬는날 아니면 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 중간 저장 (2차) ##\n",
    "## 결측치,feature추가,scaling 완료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sample.to_csv('train_sample_forecast_시간별')\n",
    "# test_sample.to_csv('test_sample_forecast_시간별')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sample = pd.read_csv('train_sample_forecast_시간별',index_col='Time',parse_dates=True) # scaling 된거\n",
    "# test_sample = pd.read_csv('test_sample_forecast_시간별',index_col='Time',parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################### Shifting #################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_step = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################### Train DATA #############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp_list = [] \n",
    "# 각 세대별 전력수요량+feature의 값을 model input 형식에 맞춤\n",
    "\n",
    "for i in total.columns[:-5]: # 임시 list에 [전력량,기온,DayOfWeek,holiDay,humidity]의 형식으로 값을 하나씩 저장\n",
    "    tmp_list.append(total[[i,'temp','DayOfWeek','Hour','holiDay','humidity']][:\"2018-06-29\"].values[:-shift_step].reshape(134,24,6)) \n",
    "    \n",
    "to_formatted_x = np.array(tmp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(191486, 24, 6)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# res_x에 모든train 세대의 데이터(전력수요량,feature) 전부 넣음\n",
    "res_x = to_formatted_x.reshape(to_formatted_x.shape[0]*to_formatted_x.shape[1],24,6)\n",
    "\n",
    "res_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> input=(전력수요량,기온,DayOfWeek,holiDay,humidity)  =>  output=(전력수요량)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_shifting_y = total.shift(-shift_step)\n",
    "\n",
    "tmp_shift_list = []\n",
    "for i in total.columns[:-5]:\n",
    "    tmp_shift_list.append(for_shifting_y[i][:\"2018-06-29\"].values[:-shift_step].reshape(134,24,1))\n",
    "\n",
    "to_formatted_y = np.array(tmp_shift_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(191486, 24, 1)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_y = to_formatted_y.reshape(to_formatted_x.shape[0]*to_formatted_x.shape[1],24,1)\n",
    "\n",
    "res_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################### TEST DATA #############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_test_list = [] \n",
    "for i in test_sample.columns[:-5]:\n",
    "    tmp_test_list.append(test_sample[[i,'temp','DayOfWeek','Hour','holiDay','humidity']]['2018-06-29'].values) \n",
    "\n",
    "to_formatted_test_x = np.array(tmp_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_test_x = to_formatted_test_x.reshape(to_formatted_test_x.shape[0]*to_formatted_test_x.shape[1],24,6)\n",
    "\n",
    "res_test_x = to_formatted_test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################### Model #############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import LSTM \n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense ,Dropout\n",
    "import keras.backend as K \n",
    "from keras.callbacks import EarlyStopping \n",
    "from keras.layers import TimeDistributed\n",
    "K.clear_session()\n",
    "\n",
    "\n",
    "model = Sequential() # Sequeatial Model \n",
    "model.add(LSTM(96, input_shape=(24, 6),return_sequences=True)) # (timestep, feature)\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(96, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(96, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(TimeDistributed(Dense(1, activation=\"linear\")))\n",
    "\n",
    "# model = Sequential()\n",
    "# for i in range(2):\n",
    "#     model.add(LSTM(64, batch_input_shape=(1, 24, 6), stateful=True, return_sequences=True))\n",
    "#     model.add(Dropout(0.3))\n",
    "# model.add(LSTM(64, batch_input_shape=(1, 24, 6), stateful=True,return_sequences=True))\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(TimeDistributed(Dense(1, activation=\"linear\")))\n",
    "\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################### Model Training #############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early_stop = EarlyStopping(monitor='loss',patience=10, verbose=1)\n",
    "\n",
    "# for i in range(1429):\n",
    "#     print('%d epoch' %i)\n",
    "#     model.fit(to_formatted_x[i], to_formatted_y[i], epochs=1, batch_size=1, shuffle=False,callbacks=[early_stop], validation_split=0.2)\n",
    "#     model.reset_states()\n",
    "    \n",
    "# model.save('mnist_mlp_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 172337 samples, validate on 19149 samples\n",
      "Epoch 1/30\n",
      " 62444/172337 [=========>....................] - ETA: 2:13 - loss: 0.0147"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-7628c2792219>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mearly_stop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mres_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m134\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "early_stop = EarlyStopping(monitor='loss',patience=5, verbose=1)\n",
    "\n",
    "history = model.fit(res_x,res_y,epochs=20,batch_size=134,verbose=1,validation_split=0.1,shuffle=True,callbacks=[early_stop])\n",
    "\n",
    "from keras.models import load_model\n",
    "model.save('mnist_mlp_model.h5')\n",
    "\n",
    "# from IPython.display import SVG\n",
    "# from keras.utils.vis_utils import model_to_dot\n",
    "# SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-22d89a5cfbf8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mhistory_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')# ‘bo’는 파란색 점\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')# ‘b’는 파란색 실선\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################### Predict #############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('mnist_mlp_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "In a stateful network, you should only pass inputs with a number of samples that can be divided by the batch size. Found: 177 samples. Batch size: 32.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-bf798964fabb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres_test_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1447\u001b[0m                                  \u001b[1;34m'divided by the batch size. Found: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1448\u001b[0m                                  \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' samples. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1449\u001b[1;33m                                  'Batch size: ' + str(batch_size) + '.')\n\u001b[0m\u001b[0;32m   1450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1451\u001b[0m         \u001b[1;31m# Prepare inputs, delegate logic to `predict_loop`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: In a stateful network, you should only pass inputs with a number of samples that can be divided by the batch size. Found: 177 samples. Batch size: 32."
     ]
    }
   ],
   "source": [
    "pred = model.predict(res_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## to inverse minmax scale\n",
    "\n",
    "col_list = test_sample.columns.insert(0,'임시')\n",
    "tt_index = test_sample[\"2018-06-30\"].index\n",
    "\n",
    "test_inverse_df = pd.DataFrame()\n",
    "\n",
    "for i in range(1,len(test_sample.columns)-4):# feature 제외\n",
    "    tmp = i*24\n",
    "    tt = pred.reshape(4248,1)[tmp-24 : tmp]    \n",
    "    test_inverse_df[col_list[i]] = tt.reshape(24,)\n",
    "    \n",
    "    test_inverse_df = test_inverse_df.set_index(tt_index)\n",
    "    test_inverse_df_array = sc_MinMax.inverse_transform(test_inverse_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inverse minmax scale DataFrame \n",
    "res_test_df = pd.DataFrame(test_inverse_df_array, columns=test_inverse_df.columns,index=tt_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측한 결과물 DataFrame\n",
    "# res_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과물과 원본 DataFrame 모두 inverse scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_none_scale = pd.read_csv('test_sample',index_col='Time',parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_none_scale = test_none_scale.loc[:,:'X230']['2018-06-30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 결과 체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 일별\n",
    "\n",
    "test_none_scale['X402'].plot(figsize=(10,8),color='red',label='관측값')\n",
    "res_test_df['X402'].plot(figsize=(10,8),color='blue',label='예측값')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### forecast#########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측값 처리못한 컬럼들 따로 예측\n",
    "\n",
    "forecast_test = pd.read_csv('forecast_test_시간별',index_col='Time',parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(total.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(forecast_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "sc_test = MinMaxScaler() # testSet scaler\n",
    "\n",
    "forecast_test_sc = sc_test.fit_transform(forecast_test.iloc[:,:-4])\n",
    "\n",
    "forecast_test_sc = np.concatenate((forecast_test_sc, temp_sc), axis=1)\n",
    "forecast_test_sc = np.concatenate((forecast_test_sc, humidity_sc), axis=1)\n",
    "forecast_test_sc = np.concatenate((forecast_test_sc, DayOfWeek_sc), axis=1)\n",
    "forecast_test_sc = np.concatenate((forecast_test_sc, Hour_sc), axis=1)\n",
    "\n",
    "forecast_test = pd.DataFrame(forecast_test_sc, columns=forecast_test.columns,index=forecast_test.index)\n",
    "\n",
    "forecast_test['holiDay'] = 0 # holiday feature 추가 / 우선 0으로 \n",
    "\n",
    "for i in range(len(forecast_test)):\n",
    "    if str(forecast_test.index[i])[:10] in holiday_list :\n",
    "        forecast_test['holiDay'].iloc[i] = 1 # 쉬는날이면 1\n",
    "    else:\n",
    "        forecast_test['holiDay'].iloc[i] = 0 # 쉬는날 아니면 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_list = ['X26', 'X16', 'X7', 'X18', 'X41', 'X55', 'X5', 'X43', 'X59',\n",
    "'X10','X62', 'X61', 'X32', 'X31', 'X30', 'X51', 'X35', 'X44',\n",
    "'X37', 'X42','X50', 'X47', 'X20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_test_list = [] \n",
    "for i in forecast_test[dropped_list]:\n",
    "    tmp_test_list.append(forecast_test[[i,'temp','DayOfWeek','Hour','holiDay','humidity']]['2018-06-30'].values) \n",
    "\n",
    "to_formatted_test_x = np.array(tmp_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_x = to_formatted_test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(forecast_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to inverse minmax scale\n",
    "\n",
    "col_list = dropped_list.insert(0,'임시')\n",
    "tt_index = forecast_test['2018-06-30'].index # 07/01 ~ 07/10\n",
    "\n",
    "test_inverse_df = pd.DataFrame()\n",
    "\n",
    "for i in range(1,len(dropped_list)):# temp col 제외\n",
    "    tmp = i*24\n",
    "    tt = pred.reshape(552,1)[tmp-24:tmp]\n",
    "    test_inverse_df[dropped_list[i]] = tt.reshape(24,)\n",
    "    \n",
    "test_inverse_df = test_inverse_df.set_index(tt_index)\n",
    "test_inverse_df_array = sc_test.inverse_transform(test_inverse_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse minmax scale DataFrame \n",
    "\n",
    "res_forecast = pd.DataFrame(test_inverse_df_array, columns=test_inverse_df.columns,index=tt_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_test['X75']['2018-06-30'].plot()\n",
    "res_test_df['X75']['2018-06-30'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X94세대 특이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# res_forecast.to_csv(\"10일 일별 예측 결과\") # 07/01 24시간"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
