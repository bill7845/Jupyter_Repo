{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = 'C:/Users/KIHyuk/Desktop/brunch_data/json'\n",
    "\n",
    "def get_file_list(dir_name): # file name들을 가져오는 함수 # 폴더명 인자 # 폴더가 위치한 경로를 인자로\n",
    "    return os.listdir(dir_name) # 폴더 내 파일명을 리스트 형태로 반환 \n",
    "\n",
    "file_list = get_file_list(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_keyword(x):\n",
    "    tmp = []\n",
    "    for val in x:\n",
    "        tmp.append(val.replace(\"\\n\",\"\").replace(\" \",\"\"))\n",
    "    return tmp\n",
    "\n",
    "def pre_comment(x):\n",
    "    if len(x) == 0:\n",
    "        return None\n",
    "    else :\n",
    "        return x\n",
    "\n",
    "def pre_text(x):\n",
    "    return str(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.DataFrame(columns=['class','text'])\n",
    "each_df = {}\n",
    "\n",
    "for file in file_list:\n",
    "    with open('C:/Users/KIHyuk/Desktop/brunch_data/json/'+file,encoding='UTF8') as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "    \n",
    "    df = pd.DataFrame(json_data['data'],\n",
    "                  columns=['title','keyword','text','nickname','publish_date','likes','share','comment','url','url_plink'])\n",
    "    df = df.dropna(subset=['text'])\n",
    "    df['keyword'] = df['keyword'].apply(pre_keyword)\n",
    "    df['comment'] = df['comment'].apply(pre_comment)\n",
    "    df['text'] = df['text'].apply(pre_text)\n",
    "    df.insert(0,\"class\",file[:-5])\n",
    "    \n",
    "    all_df = pd.concat([all_df,df[['class','text']][:500]])\n",
    "    each_df[file[:-5]] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(all_df['text'],all_df['class'],test_size=0.2 ,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n",
      "C:\\Anaconda3\\lib\\site-packages\\jpype\\_core.py:217: UserWarning: \n",
      "-------------------------------------------------------------------------------\n",
      "Deprecated: convertStrings was not specified when starting the JVM. The default\n",
      "behavior in JPype will be False starting in JPype 0.8. The recommended setting\n",
      "for new code is convertStrings=False.  The legacy value of True was assumed for\n",
      "this session. If you are a user of an application that reported this warning,\n",
      "please file a ticket with the developer.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Twitter\n",
    "\n",
    "twitter = Twitter()\n",
    "\n",
    "def tw_tokenizer(text):\n",
    "    tokens_ko = twitter.morphs(text)\n",
    "    return tokens_ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(tokenizer=tw_tokenizer, ngram_range=(1,2), min_df=3, max_df=0.9)\n",
    "tfidf_vect.fit(X_train)\n",
    "tfidf_matrix_train = tfidf_vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_clf = LogisticRegression(random_state=0)\n",
    "\n",
    "params = {'C' :[1, 3.5, 4.5, 5.5, 10]}\n",
    "\n",
    "grid_cv = GridSearchCV(lg_clf, parma_grid=params, cv=3, scoring='accuracy', verbose=1)\n",
    "grid_cv.fit(tfidf_matrix_train,y_train)\n",
    "print(grid_cv.best_params_, round(grid_cv.best_score_,4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
