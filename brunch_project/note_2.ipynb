{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "dir_name = 'C:/Users/KIHyuk/Desktop/brunch_data/json'\n",
    "\n",
    "def get_file_list(dir_name): # file name들을 가져오는 함수 # 폴더명 인자 # 폴더가 위치한 경로를 인자로\n",
    "    return os.listdir(dir_name) # 폴더 내 파일명을 리스트 형태로 반환 \n",
    "\n",
    "file_list = get_file_list(dir_name)\n",
    "\n",
    "def pre_keyword(x):\n",
    "    tmp = []\n",
    "    for val in x:\n",
    "        tmp.append(val.replace(\"\\n\",\"\").replace(\" \",\"\"))\n",
    "    return tmp\n",
    "\n",
    "def pre_comment(x):\n",
    "    if len(x) == 0:\n",
    "        return None\n",
    "    else :\n",
    "        return x\n",
    "\n",
    "def pre_text(x):\n",
    "    return str(x)\n",
    "\n",
    "def pre_datetime(x):\n",
    "    x = x.split('T')[0]\n",
    "    x = pd.to_datetime(x,format=\"%Y-%m-%d\")\n",
    "    return x\n",
    "\n",
    "class_condition = {'지구한바퀴_세계여행':0 , '그림·웹툰':1, '시사·이슈':2, 'IT_트렌드':3, '사진·촬영':4, '취향저격_영화_리뷰':5,\n",
    "                   '뮤직_인사이드':6, '육아_이야기':7, '요리·레시피':8, '건강·운동':9, '멘탈_관리_심리_탐구':10, '문화·예술':11, '건축·설계':12,\n",
    "                   '인문학·철학':13, '쉽게_읽는_역사':14, '우리집_반려동물':15, '글쓰기_코치':16, '오늘은_이런_책':16, '직장인_현실_조언':17, '스타트업_경험담':17,\n",
    "                   '디자인_스토리':18, '멋진_캘리그래피':18, '사랑·이별':19, '감성_에세이':19}\n",
    "\n",
    "all_df = pd.DataFrame(columns=['class','text'])\n",
    "each_df = {}\n",
    "\n",
    "for file in file_list:\n",
    "    with open('C:/Users/KIHyuk/Desktop/brunch_data/json/'+file,encoding='UTF8') as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "    \n",
    "    df = pd.DataFrame(json_data['data'],\n",
    "                  columns=['title','keyword','text','nickname','publish_date','likes','share','comment','url','url_plink'])\n",
    "    df = df.dropna(subset=['text'])\n",
    "    df['keyword'] = df['keyword'].apply(pre_keyword)\n",
    "    df['comment'] = df['comment'].apply(pre_comment)\n",
    "    df['text'] = df['text'].apply(pre_text)\n",
    "    df['publish_date'] = df['publish_date'].apply(pre_datetime)\n",
    "    df.insert(0,\"class\",file[:-5])\n",
    "\n",
    "    all_df = pd.concat([all_df,df[['class','title','text','keyword','likes','share','comment','publish_date','url']][:3000]])\n",
    "    each_df[file[:-5]] = df\n",
    "\n",
    "all_df['class'] = all_df['class'].map(class_condition)\n",
    "all_df['class'] = all_df['class'].astype('category')\n",
    "all_df = all_df.dropna(subset=['class'])\n",
    "all_df = all_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def pre_text_2(x):\n",
    "    pa = re.compile(\"^\\\\\\\\xa0|xa\")\n",
    "    pa1 = re.compile(r\"'http.*?'\") # 전체 url 제거\n",
    "    pa2 = re.compile(r'\\([^)]*\\)') # () 사이 문자  \n",
    "    pa3 = re.compile('[^\\w\\s]') # 특수문자 삭제\n",
    "    pa4 = re.compile(r'[^a-zA-Zㄱ-힗]')\n",
    "\n",
    "    x = re.sub(pa,' ',x)\n",
    "    x = re.sub(pa1,' ',x)\n",
    "    x = re.sub(pa2,' ',x)\n",
    "    x = re.sub(pa3, ' ',x)\n",
    "    x = re.sub(pa4, ' ',x)\n",
    "    x = x.strip()\n",
    "    x = \" \".join(x.split())\n",
    "    return x \n",
    "\n",
    "all_df['text'] = all_df['text'].apply(pre_text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train/Validation/Test 분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "from konlpy.tag import Mecab\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "X_train,X_test,y_train,y_test = train_test_split(all_df[['text']],all_df['class'],test_size=0.2,random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "mecab = Mecab()\n",
    "def mecab_tokenizer(text):\n",
    "    tokens_ko = mecab.morphs(text)\n",
    "    return tokens_ko\n",
    "\n",
    "k_stopwords = pd.read_csv(\"/content/drive/My Drive/Colab code/brunch_project/data/basement/k_stopwords.csv\",sep='\\t',header=None)\n",
    "k_stopwords = list(k_stopwords.iloc[:,0])\n",
    "\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(tokenizer=mecab_tokenizer, max_df=0.9, stop_words=k_stopwords)\n",
    "tfidf_train_matrix = tfidf_vect.fit_transform(X_train['text']) # matrix 저장\n",
    "tfidf_test_matrix = tfidf_vect.transform(X_test['text']) ## test matrix 저장\n",
    "tfidf_val_matrix = tfidf_vect.transform(X_val['text']) ## validation matrix 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_trend_chart(df,select_keyword,resampling_num):    \n",
    "    if resampling_num == 'M':\n",
    "        df = df['keyword']['2020-01-01':].resample('M').sum()\n",
    "    else:\n",
    "        df = df['keyword']['2020-01-01':].resample(resampling_num + 'D').sum()\n",
    "    \n",
    "    res_df = pd.DataFrame(columns=select_keyword,index=df.index)\n",
    "    for keyword in select_keyword:\n",
    "        keyword_week_count = []\n",
    "        for week in range(len(df)):\n",
    "            keyword_week_count.append(df.iloc[week].count(keyword))\n",
    "        res_df[keyword] = keyword_week_count\n",
    "        \n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>감성에세이</th>\n",
       "      <th>여행</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>publish_date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-15</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-29</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-02-12</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-03-11</td>\n",
       "      <td>52</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>116</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              감성에세이  여행\n",
       "publish_date           \n",
       "2020-01-01        1   0\n",
       "2020-01-15        2   0\n",
       "2020-01-29        3   0\n",
       "2020-02-12        8  22\n",
       "2020-02-26       39  39\n",
       "2020-03-11       52  69\n",
       "2020-03-25      116  44\n",
       "2020-04-08        0   0"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_trend_chart(all_df_copy,[\"감성에세이\",\"여행\"],'14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(all_df['text'],all_df['class'],test_size=0.2 ,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "def okt_tokenizer(text):\n",
    "    tokens_ko = okt.morphs(text)\n",
    "    return tokens_ko\n",
    "\n",
    "import re\n",
    "\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "train_data = []\n",
    "for sentence in X_train[:5]:\n",
    "#     temp_X = []\n",
    "    sentence = re.sub(r'[^0-9a-zA-Zㄱ-힗]',' ',sentence)\n",
    "    sentence = re.sub(r'[xa0]','',sentence)\n",
    "#     temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "#     temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    train_data.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(tokenizer=tw_tokenizer, ngram_range=(1,2), min_df=3, max_df=0.9)\n",
    "tfidf_vect.fit(X_train)\n",
    "tfidf_matrix_train = tfidf_vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_clf = LogisticRegression(random_state=0)\n",
    "\n",
    "params = {'C' :[1, 3.5, 4.5, 5.5, 10]}\n",
    "\n",
    "grid_cv = GridSearchCV(lg_clf, parma_grid=params, cv=3, scoring='accuracy', verbose=1)\n",
    "grid_cv.fit(tfidf_matrix_train,y_train)\n",
    "print(grid_cv.best_params_, round(grid_cv.best_score_,4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
